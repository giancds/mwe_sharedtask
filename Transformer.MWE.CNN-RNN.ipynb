{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.MWE.CNN-RNN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f2934cd8382e413da625beda562a5d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_496d4132111b4aeca39e3ea1f09d35ad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5332d4f3e9cc402a9ad408c0d0118c66",
              "IPY_MODEL_160703c8e4a7449d9cb0e354af2ca5b0"
            ]
          }
        },
        "496d4132111b4aeca39e3ea1f09d35ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5332d4f3e9cc402a9ad408c0d0118c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_73920eac390241fbaf275f58a8bac0e6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e57ea5c79f14ac282ddf44c27293b44"
          }
        },
        "160703c8e4a7449d9cb0e354af2ca5b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a24e90ddffc74aa4868b5339af888c15",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466/466 [00:00&lt;00:00, 1.33kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_950d7c62d3754cdd8a537e0c40d69f55"
          }
        },
        "73920eac390241fbaf275f58a8bac0e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e57ea5c79f14ac282ddf44c27293b44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a24e90ddffc74aa4868b5339af888c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "950d7c62d3754cdd8a537e0c40d69f55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87f93a0f4ac0447cb910c149e1ca85ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_40b837046e4a4d268c3433c9fd94fdd0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c6a091f71e1a4151af64fbf8bc233fac",
              "IPY_MODEL_e61f27d1da2b4bbeae8042891671a9d0"
            ]
          }
        },
        "40b837046e4a4d268c3433c9fd94fdd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6a091f71e1a4151af64fbf8bc233fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_da9afbfdf8b44e18a7c7e32e18cf07ca",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 541808922,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 541808922,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0d76c0208784a6880a02df85fd7a3b4"
          }
        },
        "e61f27d1da2b4bbeae8042891671a9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a8b12ad1e4d2481ab136d568bf406a91",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 542M/542M [00:19&lt;00:00, 27.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2bf41b7c05b0454f9799a6a6697bc840"
          }
        },
        "da9afbfdf8b44e18a7c7e32e18cf07ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0d76c0208784a6880a02df85fd7a3b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a8b12ad1e4d2481ab136d568bf406a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2bf41b7c05b0454f9799a6a6697bc840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iduyUpQLMQu_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6849c262-0051-4305-d683-e0bfee5ccc58"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErXXrRtVIB67",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "14123aee-27a8-485a-e832-a42b5bcfdd95"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul  8 13:05:06 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    58W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaOrLp1V7Thq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "f29c724f-dc05-4021-eebf-3dd5b966c196"
      },
      "source": [
        "!pip install skorch transformers \n",
        "!pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.41.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.7)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.5.0+cu101 in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: torchvision==0.6.0+cu101 in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.0+cu101) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4EcbeP-7F94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_DGhaUSB0mY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.argv = sys.argv[:1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEM3NMSb7gpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from skorch.callbacks import ProgressBar, EarlyStopping, Checkpoint\n",
        "from skorch.helper import predefined_split\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7frPRyDGEbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from eval_scripts.evaluate import Main\n",
        "\n",
        "ID_LABELS = {\n",
        "    0: '*',\n",
        "    1: 'IAV',\n",
        "    2: 'IRV',\n",
        "    3: 'LVC.cause',\n",
        "    4: 'LVC.full',\n",
        "    5: 'LS.ICV',\n",
        "    6: 'MVC',\n",
        "    7: 'VID',\n",
        "    8: 'VPC.full',\n",
        "    9: 'VPC.semi',\n",
        "}\n",
        "\n",
        "def evaluate_model(net, test_iterator, tokenizer, args):\n",
        "    preds = []\n",
        "    sents = []\n",
        "    i = 0\n",
        "    for x, y in test_iterator:\n",
        "        y_pred = net.predict(x)\n",
        "    #     i += 1\n",
        "        if i % 40 == 0:\n",
        "            print(i)\n",
        "        i += 1\n",
        "        sub_tokens = []\n",
        "        sub_preds = []\n",
        "        text = []\n",
        "        predictions = []\n",
        "        tokens = tokenizer.convert_ids_to_tokens(x.detach().cpu().numpy().reshape(-1))\n",
        "        # tokens = tokens\n",
        "        y_pred = y_pred.cpu().detach().reshape(-1).tolist()\n",
        "        for t, p in zip(tokens, y_pred):\n",
        "            if '#' in t:\n",
        "                sub_tokens.append(t.replace('#', ''))\n",
        "                sub_preds.append(p)\n",
        "            else:\n",
        "                if sub_tokens:\n",
        "                    old_token = ''.join([text[-1]] + sub_tokens)\n",
        "                    old_pred = sum(sub_preds)\n",
        "                    text = text[0:-1]\n",
        "                    text.append(old_token)\n",
        "                    predictions = predictions[0:-1]\n",
        "                    predictions.append(old_pred\n",
        "                                       if old_pred == 0\n",
        "                                       else (sub_preds[0]\n",
        "                                             if sub_preds[0] > 0\n",
        "                                             else max(sub_preds)))\n",
        "                    old_token = t\n",
        "                    old_pred = p\n",
        "                    sub_tokens = []\n",
        "                    sub_preds = []\n",
        "                else:\n",
        "                    old_token = t\n",
        "                    old_pred = p\n",
        "                text.append(old_token)\n",
        "                predictions.append(old_pred)\n",
        "                assert len(text[1:-1]) == len(predictions[1:-1])\n",
        "        sents.append(text[1:-1])\n",
        "        preds += predictions[1:-1]\n",
        "\n",
        "    binary = args.labels == 'binary'\n",
        "    output_count = 0\n",
        "    with open(args.dev_file, 'r') as dev:\n",
        "        with open(args.dev_file.replace('dev.cupt', 'temp.cupt'), 'w') as test:\n",
        "            for line in dev:\n",
        "                feats = line.split()\n",
        "                if not line.startswith('#') and line != '\\n' and '-' not in feats[0]:\n",
        "                    prediction = preds[output_count]\n",
        "                    if prediction == 0:\n",
        "                        label = '*'\n",
        "                    else:\n",
        "                        label = ID_LABELS.get(prediction, '*')\n",
        "                        # label = 1\n",
        "                    new_line = '\\t'.join(\n",
        "                        [str(f) for f in feats[0:-1]] + [str(label)] + ['\\n'])\n",
        "                    test.write(new_line)\n",
        "                    output_count += 1\n",
        "                else:\n",
        "                    test.write(line)\n",
        "\n",
        "    # post-process the file to get the predictions into cupt format\n",
        "    with open(args.dev_file.replace('dev.cupt', 'temp.cupt'), 'r') as temp:\n",
        "        with open(args.dev_file.replace('dev.cupt', 'system.cupt'), 'w') as test:\n",
        "            current_prediction = [1, None]\n",
        "            verb_found = False\n",
        "            for line in temp:\n",
        "                feats = line.split('\\t')\n",
        "                if not line.startswith('#') and line != '\\n' and '-' not in feats[0]:\n",
        "\n",
        "                    if feats[10] == '*':\n",
        "                        test.write(line)\n",
        "                        # print(line)\n",
        "                    else:\n",
        "\n",
        "                        if current_prediction[1] is None:\n",
        "\n",
        "                            label = '{}:{}'.format(current_prediction[0], feats[10])\n",
        "                            # label = str(current_prediction[0])\n",
        "                            verb_found = True if feats[3] == 'VERB' else False\n",
        "                            current_prediction[1] = feats[10]\n",
        "\n",
        "                        else:\n",
        "\n",
        "                            if feats[10] == current_prediction[1]:\n",
        "\n",
        "                                if verb_found and feats[3] != 'VERB':\n",
        "                                    label = current_prediction[0]\n",
        "\n",
        "                                elif verb_found and feats[3] == 'VERB':\n",
        "                                    current_prediction[0] = current_prediction[0] + 1\n",
        "                                    current_prediction[1] = feats[10]\n",
        "                                    label = '{}:{}'.format(current_prediction[0], feats[10])\n",
        "                                    # label = str(current_prediction[0])\n",
        "\n",
        "                                elif not verb_found:\n",
        "                                    label = current_prediction[0]\n",
        "                                    verb_found = True if feats[3] == 'VERB' else False\n",
        "\n",
        "                            else:\n",
        "                                current_prediction[0] = current_prediction[0] + 1\n",
        "                                current_prediction[1] = feats[10]\n",
        "                                label = '{}:{}'.format(current_prediction[0], feats[10])\n",
        "                                # label = str(current_prediction[0])\n",
        "                                verb_found = True if feats[3] == 'VERB' else False\n",
        "                        new_line = '\\t'.join(feats[0:-2] + [str(label)] + ['\\n'])\n",
        "                        test.write(new_line)\n",
        "                        # print(new_line)\n",
        "                else:\n",
        "                    if line == '\\n':\n",
        "                        current_prediction = [1, None]\n",
        "                        verb_found = False\n",
        "                    test.write(line)\n",
        "                    # print(line)\n",
        "\n",
        "    if args.eval:\n",
        "        _run_sript(args)\n",
        "\n",
        "def _run_sript(args):\n",
        "\n",
        "    args.debug = False\n",
        "    args.combinatorial = True\n",
        "    args.gold_file = open(args.dev_file, 'r')\n",
        "    args.prediction_file = open(args.dev_file.replace('dev.cupt', 'system.cupt'), 'r')\n",
        "    args.train_file = open(args.dev_file.replace('dev.cupt', 'train.cupt'), 'r')\n",
        "    args.debug = False\n",
        "    print('\\n\\nRunning shared-task eval script\\n\\n')\n",
        "    Main(args).run()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af-eSpyXFM45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features = {\n",
        "    '*': 0,\n",
        "    'IAV': 1,\n",
        "    'IRV': 2,\n",
        "    'LVC.cause': 3,\n",
        "    'LVC.full': 4,\n",
        "    'LS.ICV': 5,\n",
        "    'MVC': 6,\n",
        "    'VID': 7,\n",
        "    'VPC.full': 8,\n",
        "    'VPC.semi': 9,\n",
        "}\n",
        "\n",
        "\n",
        "def load_tokenized_data(datafile,\n",
        "                        language_codes,\n",
        "                        percent=1.0,\n",
        "                        seed=42,\n",
        "                        binary=False,\n",
        "                        split=True):\n",
        "\n",
        "    with open(datafile, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    x_train, y_train = [], []\n",
        "    x_val, y_val = [], []\n",
        "    x_dev, y_dev = {}, {}\n",
        "    for code in language_codes:\n",
        "\n",
        "        true_x, true_y = [], []\n",
        "        false_x, false_y = [], []\n",
        "        for i, (xsample, ysample) in enumerate(\n",
        "                zip(data[code]['x_train'], data[code]['y_train'])):\n",
        "\n",
        "            if sum(ysample) > 0:\n",
        "                true_x.append(xsample)\n",
        "                if binary:\n",
        "                    ysample = [0 if y == 0 else 1 for y in ysample]\n",
        "                true_y.append(ysample)\n",
        "\n",
        "        max_len = max([len(y) for y in true_y])\n",
        "        for xsample, ysample in zip(data[code]['x_train'],\n",
        "                                    data[code]['y_train']):\n",
        "            if sum(ysample) == 0 and len(ysample) < max_len:\n",
        "                false_x.append(xsample)\n",
        "                false_y.append(ysample)\n",
        "\n",
        "        false_x = np.array(false_x)\n",
        "        false_y = np.array(false_y)\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        idx = np.random.randint(len(false_y), size=int(percent * len(true_y)))\n",
        "        false_x = false_x[idx].tolist()\n",
        "        false_y = false_y[idx].tolist()\n",
        "\n",
        "        x_train += true_x + false_x\n",
        "        y_train += true_y + false_y\n",
        "\n",
        "        x_dev[code] = data[code][\"x_dev\"]\n",
        "        y_dev[code] = data[code][\"y_dev\"]\n",
        "\n",
        "\n",
        "    if split:\n",
        "        x_train, x_val, y_train, y_val = train_test_split(x_train,\n",
        "                                                          y_train,\n",
        "                                                          random_state=seed,\n",
        "                                                          test_size=0.15,\n",
        "                                                          shuffle=True)\n",
        "    else:\n",
        "        x_val, y_val = [], []\n",
        "        for code in language_codes:\n",
        "            x_val += data[code][\"x_dev\"]\n",
        "            y_val += data[code][\"y_dev\"]\n",
        "\n",
        "        \n",
        "    del data\n",
        "\n",
        "    return (x_train, y_train), (x_val, y_val), (x_dev, y_dev)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecaCoKGYFsEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Utilities module. \"\"\"\n",
        "\n",
        "\n",
        "def build_model_name(args, model='rnn-cnn'):\n",
        "    name = ''\n",
        "    if model == 'rnn':\n",
        "        name = (\"{0}.{1}.{2}layers.{3}lstm.{4}dropout.{5}init.{6}activation\"\n",
        "                \"{7}clipnorm.{8}batch.{9}epochs\".format(\n",
        "                    args.bert_type, args.metric, args.nlayers, args.lstm_size,\n",
        "                    args.dropout, args.initrange, args.output_activation,\n",
        "                    args.clipnorm, args.batch_size, args.max_epochs\n",
        "                ))\n",
        "    elif model == 'cnn':\n",
        "        name = (\"{0}.{1}.{2}filters.{3}kernels.{4}poolstride.{5}dropout.\"\n",
        "                \"{6}activation.{7}batch.{8}epochs\".format(\n",
        "                    args.bert_type, args.metric, args.nfilters, args.kernels,\n",
        "                    args.pool_stride, args.dropout, args.output_activation,\n",
        "                    args.batch_size, args.max_epochs\n",
        "                ))\n",
        "    elif model == 'nn':\n",
        "        name = (\"{0}.{1}.{2}hidden.{3}dropout.{4}activation.{5}batch.{6}epochs\".format(\n",
        "                    args.bert_type, args.metric, args.hidden_size, args.dropout,\n",
        "                    args.output_activation, args.batch_size, args.max_epochs\n",
        "                ))\n",
        "    elif model == 'cnn-rnn':\n",
        "         name = (\"{0}.{1}.{2}filters.{3}kernels.{4}poolstride.{5}layers.\"\n",
        "                \"{6}lstm.{7}dropout.{8}init.{9}activation.{10}batch.\"\n",
        "                \"{11}epochs\".format(\n",
        "                    args.bert_type, args.metric, args.nfilters, args.kernels,\n",
        "                    args.pool_stride, args.nlayers, args.lstm_size, args.dropout,\n",
        "                    args.initrange, args.output_activation, args.batch_size,\n",
        "                    args.max_epochs\n",
        "                ))\n",
        "    return name"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGpej-dKE5Nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import skorch\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from torchtext.data import Dataset, Field, Example, BucketIterator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "class SkorchBucketIterator(BucketIterator):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dataset,\n",
        "                 batch_size,\n",
        "                 sort_key=None,\n",
        "                 device=None,\n",
        "                 batch_size_fn=None,\n",
        "                 train=True,\n",
        "                 repeat=False,\n",
        "                 shuffle=None,\n",
        "                 sort=None,\n",
        "                 sort_within_batch=None,\n",
        "                 one_hot=True,\n",
        "                 num_classes=2):\n",
        "        self.one_hot = one_hot\n",
        "        self.num_classes = num_classes\n",
        "        super(SkorchBucketIterator,\n",
        "              self).__init__(dataset, batch_size, sort_key, device,\n",
        "                             batch_size_fn, train, repeat, shuffle, sort,\n",
        "                             sort_within_batch)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in super().__iter__():\n",
        "            # We make a small modification: Instead of just returning batch\n",
        "            # we return batch.text and batch.label, corresponding to X and y\n",
        "            # if self.train:\n",
        "            if self.one_hot:\n",
        "                y = batch.labels.to('cpu')\n",
        "                y = to_categorical(y, num_classes=self.num_classes)\n",
        "                y = torch.tensor(y).to(self.device)\n",
        "                batch.labels = y\n",
        "            else:\n",
        "                batch.labels = batch.labels.float()\n",
        "            yield batch.sentence, batch.labels\n",
        "\n",
        "\n",
        "class SentenceDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, min_len=5, **kwargs):\n",
        "        self.min_len = min_len\n",
        "        text_field = Field(use_vocab=False, pad_token=0, batch_first=True)\n",
        "        label_field = Field(use_vocab=False, pad_token=-1, batch_first=True)\n",
        "        fields = [(\"sentence\", text_field), (\"labels\", label_field)]\n",
        "        examples = []\n",
        "        for (x, y) in zip(data[0], data[1]):\n",
        "            if len(x) < self.min_len:     # pad all sequences shorter than this\n",
        "                x += [0] * (5 - len(x))\n",
        "                y += [-1] * (5 - len(y))\n",
        "            examples.append(Example.fromlist([x, y], fields))\n",
        "        super().__init__(examples, fields, **kwargs)\n",
        "\n",
        "\n",
        "class IdiomClassifier(skorch.NeuralNetClassifier):\n",
        "\n",
        "    def __init__(self, print_report=True, class_weights=None, score_average='binary', *args, **kwargs):\n",
        "        self.print_report = print_report\n",
        "        self.class_weights = class_weights\n",
        "        self.score_average = score_average\n",
        "        if class_weights is None:\n",
        "            self.class_weights = [1.0, 1.0]\n",
        "        super(IdiomClassifier, self).__init__(*args, **kwargs)\n",
        "        self.set_params(callbacks__valid_acc=None)\n",
        "        self.set_params(criterion__reduction='none')\n",
        "\n",
        "    def get_loss(self, y_pred, y_true, X, *args, **kwargs):\n",
        "        if isinstance(self.criterion_, torch.nn.BCELoss):\n",
        "            loss = super().get_loss(\n",
        "                y_pred.view(-1), y_true.view(-1), X, *args, **kwargs)\n",
        "        else:\n",
        "            if isinstance(self.criterion_, torch.nn.NLLLoss):\n",
        "                y_pred = self.module.output_activation(y_pred, dim=2)\n",
        "            loss = super().get_loss(\n",
        "                y_pred.view(-1, self.module.noutputs),\n",
        "                y_true.long().view(-1), X, *args, **kwargs)\n",
        "\n",
        "        if self.class_weights is not None:\n",
        "            weights = torch.ones_like(y_true) * y_true\n",
        "            for w, weight in enumerate(self.class_weights):\n",
        "                weights = torch.where(\n",
        "                    y_true == w,\n",
        "                    torch.tensor(weight).float().to(self.device),\n",
        "                    weights)\n",
        "            loss = (loss * weights.view(-1))\n",
        "        if isinstance(self.criterion_, torch.nn.BCELoss):\n",
        "            mask = (y_true >= 0).int()\n",
        "            loss = (loss * mask.view(-1))\n",
        "        return loss.mean()\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        self.module.eval()\n",
        "        y_pred = self.module(X)\n",
        "\n",
        "        if self.module.output_activation == 'softmax':\n",
        "            y_pred = F.softmax(y_pred, dim=2)\n",
        "        else:\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.predict_proba(X)\n",
        "\n",
        "        if self.module.noutputs > 1:\n",
        "            y_pred = torch.argmax(y_pred, dim=2)\n",
        "        else:\n",
        "            y_pred = (y_pred > 0.5).int()\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def score(self, X, y=None):\n",
        "        self.module.eval()\n",
        "        ds = self.get_dataset(X)\n",
        "        target_iterator = self.get_iterator(ds, training=False)\n",
        "\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for x, y in target_iterator:\n",
        "            preds = self.predict(x)\n",
        "            y_pred.append(preds.view(-1))\n",
        "            if len(y.shape) > 2:\n",
        "                y = torch.argmax(y, dim=2)\n",
        "            y_true.append(y.view(-1))\n",
        "        y_true = torch.cat(y_true).cpu().view(-1).detach().numpy().tolist()\n",
        "        y_pred = torch.cat(y_pred).cpu().view(-1).detach().numpy().tolist()\n",
        "\n",
        "        tt, tp = [], []\n",
        "        for t, p in zip(y_true, y_pred):\n",
        "            if t >= 0:\n",
        "                tt.append(t)\n",
        "                tp.append(p)\n",
        "\n",
        "        y_true = tt\n",
        "        y_pred = tp\n",
        "\n",
        "        if self.print_report:\n",
        "            print('Confusion matrix')\n",
        "            print(confusion_matrix(y_true, y_pred))\n",
        "            print(classification_report(y_true, y_pred))\n",
        "        return f1_score(y_true, y_pred, average=self.score_average)\n",
        "\n",
        "\n",
        "class CustomScorer(skorch.callbacks.EpochScoring):\n",
        "\n",
        "    def on_epoch_end(self, net, dataset_train, dataset_valid, **kwargs):\n",
        "        current_score = net.score(dataset_valid)\n",
        "        self._record_score(net.history, current_score)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvcm3d6JFdp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NNClassifier(nn.Module):\n",
        "    def __init__(self, config, transformer, transformer_device):\n",
        "        super(NNClassifier, self).__init__()\n",
        "\n",
        "        self.transformer_device = transformer_device\n",
        "        self.model_device = transformer_device\n",
        "        self.transformer = transformer\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.ninputs = transformer.embeddings.word_embeddings.embedding_dim\n",
        "        if config.hidden_size > 0:\n",
        "            self.fully_connected1 = nn.Linear(self.ninputs, config.hidden_size)\n",
        "            ninputs_to_classifier = config.hidden_size\n",
        "        else:\n",
        "            self.fully_connected1 = None\n",
        "            ninputs_to_classifier = self.ninputs\n",
        "\n",
        "        self.noutputs = 1\n",
        "        if config.labels == 'multilabel':\n",
        "            self.noutputs = config.num_outputs\n",
        "        else:\n",
        "            if config.output_activation == 'softmax':\n",
        "                self.noutputs = 2\n",
        "\n",
        "        self.fully_connected = nn.Linear(ninputs_to_classifier, self.noutputs)\n",
        "\n",
        "        self.output_activation = ('sigmoid'  # pylint: disable=no-member\n",
        "                                  if self.noutputs == 1\n",
        "                                  else ('sigmoid'\n",
        "                                        if config.output_activation == 'sigmoid'\n",
        "                                        else 'softmax'))\n",
        "\n",
        "    def to(self, *args, **kwargs):\n",
        "        self = super().to(*args, **kwargs)\n",
        "        self.transformer = self.transformer.to(\n",
        "            torch.device(self.transformer_device))\n",
        "        self.model_device = next(self.fully_connected.parameters()).device.type\n",
        "        return self\n",
        "\n",
        "    def freeze_transformer(self):\n",
        "        for param in self.transformer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_transformer(self):\n",
        "        for param in self.transformer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.transformer_device)\n",
        "        m = (x > 0).int()\n",
        "        x = self.transformer(x, attention_mask=m)[0]\n",
        "        #\n",
        "        #\n",
        "        if self.transformer_device != self.model_device:\n",
        "            x = x.to(self.model_device)\n",
        "\n",
        "        if self.fully_connected1 is not None:\n",
        "            x = F.elu(self.fully_connected1(self.dropout(x)))\n",
        "\n",
        "        return self.fully_connected(self.dropout(x))\n",
        "\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, config, transformer, transformer_device):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "\n",
        "        self.transformer_device = transformer_device\n",
        "        self.model_device = transformer_device\n",
        "        self.transformer = transformer\n",
        "        self.convolutions = nn.ModuleList([\n",
        "            nn.Conv1d(\n",
        "                in_channels=transformer.embeddings.word_embeddings.embedding_dim,\n",
        "                out_channels=config.nfilters,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=1) for kernel_size in config.kernels])\n",
        "\n",
        "        self.pool_stride = config.pool_stride\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        ninputs = (config.nfilters // config.pool_stride) * len(config.kernels)\n",
        "\n",
        "        self.noutputs = 1\n",
        "        if config.labels == 'multilabel':\n",
        "            self.noutputs = config.num_outputs\n",
        "        else:\n",
        "            if config.output_activation == 'softmax':\n",
        "                self.noutputs = 2\n",
        "\n",
        "        self.fully_connected = nn.Linear(ninputs, self.noutputs)\n",
        "\n",
        "        self.output_activation = ('sigmoid'  # pylint: disable=no-member\n",
        "                                  if self.noutputs == 1\n",
        "                                  else ('sigmoid'\n",
        "                                        if config.output_activation == 'sigmoid'\n",
        "                                        else 'softmax'))\n",
        "\n",
        "    def to(self, *args, **kwargs):\n",
        "        self = super().to(*args, **kwargs)\n",
        "        self.transformer = self.transformer.to(\n",
        "            torch.device(self.transformer_device))\n",
        "        self.model_device = next(self.fully_connected.parameters()).device.type\n",
        "        return self\n",
        "\n",
        "    def freeze_transformer(self):\n",
        "        for param in self.transformer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_transformer(self):\n",
        "        for param in self.transformer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.transformer_device)\n",
        "        m = (x > 0).int()\n",
        "        x = self.transformer(x, attention_mask=m)[0].transpose(1, 2)\n",
        "        #\n",
        "        seq_len = x.shape[-1]\n",
        "        #\n",
        "        if self.transformer_device != self.model_device:\n",
        "            x = x.to(self.model_device)\n",
        "        #\n",
        "        x = [F.elu(conv(x)).transpose(1, 2) for conv in self.convolutions]\n",
        "        x = [nn.functional.pad(i, (0, 0, 0, seq_len - i.shape[1])) for i in x]\n",
        "        x = [F.max_pool1d(c, self.pool_stride) for c in x]\n",
        "        x = torch.cat(x, dim=2)  # pylint: disable=no-member\n",
        "\n",
        "        return self.fully_connected(self.dropout(x))\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, config, transformer, transformer_device):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "\n",
        "        self.transformer_device = transformer_device\n",
        "        self.model_device = transformer_device\n",
        "        self.transformer = transformer\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=transformer.embeddings.word_embeddings.embedding_dim,\n",
        "            hidden_size=config.lstm_size,\n",
        "            num_layers=config.nlayers,\n",
        "            batch_first=True,\n",
        "            dropout=config.dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.noutputs = 1\n",
        "        if config.labels == 'multilabel':\n",
        "            self.noutputs = config.num_outputs\n",
        "        else:\n",
        "            if config.output_activation == 'softmax':\n",
        "                self.noutputs = 2\n",
        "\n",
        "        self.fully_connected = nn.Linear(config.lstm_size, self.noutputs)\n",
        "\n",
        "        self.output_activation = ('sigmoid'  # pylint: disable=no-member\n",
        "                                  if self.noutputs == 1\n",
        "                                  else ('sigmoid'\n",
        "                                        if config.output_activation == 'sigmoid'\n",
        "                                        else 'softmax'))\n",
        "        self.init_weights(config.initrange)\n",
        "\n",
        "    def to(self, *args, **kwargs):\n",
        "        self = super().to(*args, **kwargs)\n",
        "        self.transformer = self.transformer.to(\n",
        "            torch.device(self.transformer_device))\n",
        "        self.model_device = next(self.fully_connected.parameters()).device.type\n",
        "        return self\n",
        "\n",
        "    def freeze_transformer(self):\n",
        "        for param in self.transformer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_transformer(self):\n",
        "        for param in self.transformer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.transformer_device)\n",
        "        m = (x > 0).int()\n",
        "        x = self.transformer(x, attention_mask=m)[0]\n",
        "        #\n",
        "        seq_len = x.shape[-1]\n",
        "        #\n",
        "        if self.transformer_device != self.model_device:\n",
        "            x = x.to(self.model_device)\n",
        "        #\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        return self.fully_connected(self.dropout(x))\n",
        "\n",
        "    def init_weights(self, initrange):\n",
        "        for names in self.lstm._all_weights:\n",
        "            for name in filter(lambda n: \"bias\" in n, names):\n",
        "                bias = getattr(self.lstm, name)\n",
        "                n = bias.size(0)\n",
        "                start, end = n//4, n//2\n",
        "                bias.data[start:end].fill_(1.)\n",
        "            for name in filter(lambda n: \"weight\" in n,  names):\n",
        "                weight = getattr(self.lstm, name)\n",
        "                weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        self.fully_connected.bias.data.fill_(0)\n",
        "        self.fully_connected.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "\n",
        "class CNNRNNClassifier(nn.Module):\n",
        "    def __init__(self, config, transformer, transformer_device):\n",
        "        super(CNNRNNClassifier, self).__init__()\n",
        "\n",
        "        self.transformer_device = transformer_device\n",
        "        self.model_device = transformer_device\n",
        "        self.transformer = transformer\n",
        "\n",
        "        self.convolutions = nn.ModuleList([\n",
        "            nn.Conv1d(\n",
        "                in_channels=transformer.embeddings.word_embeddings.embedding_dim,\n",
        "                out_channels=config.nfilters,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=1) for kernel_size in config.kernels])\n",
        "\n",
        "        self.pool_stride = config.pool_stride\n",
        "\n",
        "        ninputs = (config.nfilters // config.pool_stride) * len(config.kernels)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=ninputs,\n",
        "            hidden_size=config.lstm_size,\n",
        "            num_layers=config.nlayers,\n",
        "            batch_first=True,\n",
        "            dropout=config.dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.noutputs = 1\n",
        "        if config.labels == 'multilabel':\n",
        "            self.noutputs = config.num_outputs\n",
        "        else:\n",
        "            if config.output_activation == 'softmax':\n",
        "                self.noutputs = 2\n",
        "        self.fully_connected = nn.Linear(config.lstm_size, self.noutputs)\n",
        "\n",
        "        self.output_activation = ('sigmoid'  # pylint: disable=no-member\n",
        "                                  if self.noutputs == 1\n",
        "                                  else ('sigmoid'\n",
        "                                        if config.output_activation == 'sigmoid'\n",
        "                                        else 'softmax'))\n",
        "        self.init_weights(config.initrange)\n",
        "\n",
        "    def to(self, *args, **kwargs):\n",
        "        self = super().to(*args, **kwargs)\n",
        "        self.transformer = self.transformer.to(\n",
        "            torch.device(self.transformer_device))\n",
        "        self.model_device = next(self.fully_connected.parameters()).device.type\n",
        "        return self\n",
        "\n",
        "    def freeze_transformer(self):\n",
        "        for param in self.transformer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_transformer(self):\n",
        "        for param in self.transformer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.transformer_device)\n",
        "        m = (x > 0).int()\n",
        "        x = self.transformer(x, attention_mask=m)[0].transpose(1, 2)\n",
        "        #\n",
        "        seq_len = x.shape[-1]\n",
        "        if self.transformer_device != self.model_device:\n",
        "            x = x.to(self.model_device)\n",
        "        #\n",
        "        x = [F.elu(conv(x)).transpose(1, 2) for conv in self.convolutions]\n",
        "        x = [nn.functional.pad(i, (0, 0, 0, seq_len - i.shape[1])) for i in x]\n",
        "        x = [F.max_pool1d(c, self.pool_stride) for c in x]\n",
        "        x = torch.cat(x, dim=2)  # pylint: disable=no-member\n",
        "        x = self.dropout(x)\n",
        "        #\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fully_connected(self.dropout(x))\n",
        "\n",
        "    def init_weights(self, initrange):\n",
        "        for conv in self.convolutions:\n",
        "            conv.weight.data.uniform_(-initrange, initrange)\n",
        "            conv.bias.data.fill_(0)\n",
        "        for names in self.lstm._all_weights:\n",
        "            for name in filter(lambda n: \"bias\" in n, names):\n",
        "                bias = getattr(self.lstm, name)\n",
        "                n = bias.size(0)\n",
        "                start, end = n//4, n//2\n",
        "                bias.data[start:end].fill_(1.)\n",
        "            for name in filter(lambda n: \"weight\" in n,  names):\n",
        "                weight = getattr(self.lstm, name)\n",
        "                weight.data.uniform_(-initrange, initrange)\n",
        "        self.fully_connected.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fully_connected.bias.data.fill_(0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NIPFLxs7grp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9c05487-beef-4ecc-c595-87518abed3e0"
      },
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')     # pylint: disable=no-member\n",
        "LANGUAGE_CODES = ['DE', 'EL', 'EU', 'FR', 'GA', 'HE', 'HI', 'IT', 'PL', 'PT', 'RO', 'SV', 'TR',  'ZH']\n",
        "# LANGUAGE_CODES = ['DE',  'GA',  'HI', 'PT',  'ZH']\n",
        "CWD = os.getcwd()\n",
        "BASE_DIR = '/content/gdrive/My Drive/mwe_sharedtask/data'     # this will point to the user's home\n",
        "TRAIN_DIR = \"transformer/cnn\"\n",
        "DEVICE"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KMyS3WM7g5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "29266e15-e91a-4ef5-ed98-63d725c9455a"
      },
      "source": [
        "parser = argparse.ArgumentParser(description='Classifier using CNNs')\n",
        "parser.add_argument(\n",
        "    '--bert_type',\n",
        "    type=str,\n",
        "    default='distilbert-base-multilingual-cased',\n",
        "    help='transormer model [should be a miltilingual model]')\n",
        "parser.add_argument(\n",
        "    '--bert_device',\n",
        "    type=str,\n",
        "    default='gpu',\n",
        "    help='device to run the transformer model')\n",
        "parser.add_argument(\n",
        "    '--labels',\n",
        "    type=str,\n",
        "    default='multilabel',\n",
        "    help='multilabel or binary classification')\n",
        "parser.add_argument(\n",
        "    '--metric',\n",
        "    type=str,\n",
        "    default='f1',\n",
        "    help='sklearn metric to evaluate the model while training')\n",
        "parser.add_argument(\n",
        "    '--nfilters',\n",
        "    type=int,\n",
        "    default=768,\n",
        "    help='number of convolution filters')\n",
        "parser.add_argument(\n",
        "    '--kernels',\n",
        "    type=list,\n",
        "    default=[1, 2, 3, 4, 5],\n",
        "    help='number of convolution filters')\n",
        "parser.add_argument(\n",
        "    '--pool_stride',\n",
        "    type=int,\n",
        "    default=5,\n",
        "    help='size of the stride for the pooling operation')\n",
        "parser.add_argument(\n",
        "    '--hidden_size',\n",
        "    type=int,\n",
        "    default=100,\n",
        "    help='size of pre-classifier in case of feedforward')\n",
        "parser.add_argument(\n",
        "    '--nlayers',\n",
        "    type=int,\n",
        "    default=2,\n",
        "    help='number of convolution filters')\n",
        "parser.add_argument(\n",
        "    '--lstm_size',\n",
        "    type=int,\n",
        "    default=100,\n",
        "    help='number of convolution filters')\n",
        "parser.add_argument(\n",
        "    '--dropout',\n",
        "    type=float,\n",
        "    default=0.5,\n",
        "    help='dropout probability for the dense layer')\n",
        "parser.add_argument(\n",
        "    '--initrange',\n",
        "    type=float,\n",
        "    default=0.1,\n",
        "    help='range to initialize the lstm layers')\n",
        "parser.add_argument(\n",
        "    '--clipnorm',\n",
        "    type=float,\n",
        "    default=5.0,\n",
        "    help='limit to clip the l2 norm of gradients')\n",
        "parser.add_argument(\n",
        "    '--output_activation',\n",
        "    type=str,\n",
        "    default='softmax',\n",
        "    help='output activation')\n",
        "parser.add_argument(\n",
        "    '--batch_size',\n",
        "    type=int,\n",
        "    default=32,\n",
        "    help='training batch size')\n",
        "parser.add_argument(\n",
        "    '--eval_batch_size',\n",
        "    type=int,\n",
        "    default=1,\n",
        "    help='validation/evaluation batch size')\n",
        "parser.add_argument(\n",
        "    '--max_epochs',\n",
        "    type=int,\n",
        "    default=100,\n",
        "    help='max number of epochs to train the model')\n",
        "parser.add_argument(\n",
        "    \"--train_dir\",\n",
        "    type=str,\n",
        "    default=os.path.join(BASE_DIR, TRAIN_DIR) + \"/\",\n",
        "    help=\"Train dir\")\n",
        "parser.add_argument(\n",
        "    \"--eval\",\n",
        "    action=\"store_true\",\n",
        "    help=\"eval at the end of the training process\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['--eval'], dest='eval', nargs=0, const=True, default=False, type=None, choices=None, help='eval at the end of the training process', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iVJ5HpK7hC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a0c2329-836c-4cfb-b9ac-7714cdf187bb"
      },
      "source": [
        "args = parser.parse_args()\n",
        "args.kernels = [int(i) for i in args.kernels if ',' not in str(i)]\n",
        "transformer_device = torch.device(\n",
        "    'cuda' if torch.cuda.is_available() and args.bert_device == 'gpu'\n",
        "    else 'cpu')\n",
        "\n",
        "ONE_HOT_OUTPUT = False #args.output_activation == 'softmax' and args.labels == 'binary'\n",
        "transformer_device"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtYM8yfG7g25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4235dfb-aa49-4e90-89db-7d988c2bc73f"
      },
      "source": [
        "(x_train, y_train), (x_val, y_val), (x_dev, y_dev) = load_tokenized_data(\n",
        "    datafile='{}/{}{}.tokenized.all.pkl'.format(BASE_DIR, args.bert_type, '' if args.labels == 'binary' else '.multilabel'),\n",
        "    language_codes=LANGUAGE_CODES, percent=0.0, split=False,\n",
        "    seed=SEED)\n",
        "\n",
        "targets = np.concatenate(y_train).reshape(-1)\n",
        "class_weights = compute_class_weight(class_weight='balanced',\n",
        "                                     classes=np.unique(targets),\n",
        "                                     y=targets)\n",
        "args.num_outputs = len(class_weights)\n",
        "args.num_outputs"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLsY7WhWXMj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "706fbe0b-0e85-49d0-8b29-d2851ae80477"
      },
      "source": [
        "class_weights"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.11256854e-01, 1.98904153e+02, 8.01573662e+00, 3.47987189e+01,\n",
              "       2.45680017e+00, 2.79995846e+03, 2.29504792e+01, 3.02331141e+00,\n",
              "       3.44626586e+01, 2.35138630e+01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVOy-UELWUbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9afb65b8-4aac-4023-d873-9be506384260"
      },
      "source": [
        "np.unique(targets)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1zspe5Z7g15",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "f2934cd8382e413da625beda562a5d7f",
            "496d4132111b4aeca39e3ea1f09d35ad",
            "5332d4f3e9cc402a9ad408c0d0118c66",
            "160703c8e4a7449d9cb0e354af2ca5b0",
            "73920eac390241fbaf275f58a8bac0e6",
            "8e57ea5c79f14ac282ddf44c27293b44",
            "a24e90ddffc74aa4868b5339af888c15",
            "950d7c62d3754cdd8a537e0c40d69f55",
            "87f93a0f4ac0447cb910c149e1ca85ec",
            "40b837046e4a4d268c3433c9fd94fdd0",
            "c6a091f71e1a4151af64fbf8bc233fac",
            "e61f27d1da2b4bbeae8042891671a9d0",
            "da9afbfdf8b44e18a7c7e32e18cf07ca",
            "e0d76c0208784a6880a02df85fd7a3b4",
            "a8b12ad1e4d2481ab136d568bf406a91",
            "2bf41b7c05b0454f9799a6a6697bc840"
          ]
        },
        "outputId": "e99ed8a0-e165-415e-87ab-3bee92e3bc3f"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(args.bert_type)\n",
        "transformer = AutoModel.from_pretrained(args.bert_type, force_download=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2934cd8382e413da625beda562a5d7f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87f93a0f4ac0447cb910c149e1ca85ec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=541808922.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB79j7xQ7g01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2c3d4cd-c13f-4f47-92b5-3d02775d8f79"
      },
      "source": [
        "model = CNNRNNClassifier(args, transformer, transformer_device)\n",
        "model_name = build_model_name(args, model='cnn-rnn')\n",
        "model.to(DEVICE)     # pylint: disable=no-member\n",
        "model.freeze_transformer()\n",
        "print()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OgdFwcZLdcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric = 'valid_loss'\n",
        "lower_is_better = True if metric == 'valid_loss' else False\n",
        "\n",
        "progress_bar = ProgressBar(batches_per_epoch=len(x_train) // args.batch_size + 1)\n",
        "scorer = CustomScorer(scoring=None, name='F1', lower_is_better=False, use_caching=False)\n",
        "early_stopping =  EarlyStopping(monitor=metric, patience=20, lower_is_better=lower_is_better)\n",
        "checkpoint = Checkpoint(\n",
        "    monitor='{}_best'.format(metric),\n",
        "    dirname=args.train_dir,\n",
        "    f_params='{}.params.pt'.format(model_name),\n",
        "    f_optimizer='{}.optimizer.pt'.format(model_name),\n",
        "    f_history='{}.history.json'.format(model_name))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsGdaUd-7gzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = IdiomClassifier(\n",
        "    module=model,\n",
        "    class_weights=class_weights,\n",
        "    print_report=False,\n",
        "    score_average='weighted',\n",
        "     #\n",
        "    iterator_train=SkorchBucketIterator,\n",
        "    iterator_train__batch_size=args.batch_size,\n",
        "    iterator_train__sort_key=lambda x: len(x.sentence),\n",
        "    iterator_train__shuffle=True,\n",
        "    iterator_train__device=DEVICE,\n",
        "    iterator_train__one_hot=ONE_HOT_OUTPUT,\n",
        "     #\n",
        "    iterator_valid=SkorchBucketIterator,\n",
        "    iterator_valid__batch_size=args.eval_batch_size,\n",
        "    iterator_valid__sort_key=lambda x: len(x.sentence),\n",
        "    iterator_valid__shuffle=True,\n",
        "    iterator_valid__device=DEVICE,\n",
        "    iterator_valid__one_hot=ONE_HOT_OUTPUT,\n",
        "\n",
        "    train_split=predefined_split(SentenceDataset(data=(x_val, y_val))),\n",
        "    optimizer=torch.optim.SGD,\n",
        "    criterion=nn.BCELoss if args.labels == 'binary' else nn.CrossEntropyLoss,\n",
        "    criterion__ignore_index=-1,\n",
        "    criterion__reduction='none',\n",
        "    callbacks=[progress_bar, scorer, early_stopping, checkpoint],\n",
        "    # callbacks=[scorer, early_stopping, checkpoint],\n",
        "    device=DEVICE,\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgUU4f0V7gwd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e3151f0-88cc-442a-bdbe-2f836ee5ca54"
      },
      "source": [
        "net.fit(SentenceDataset(data=(x_train, y_train)), y=None, epochs=args.max_epochs)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 10/1198 [00:04<08:22,  2.37it/s, train_loss=0.545]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrthjRiPLUL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.initialize()\n",
        "net.load_params(checkpoint=checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxkiOcTagyDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(model_name)\n",
        "net.print_report = True\n",
        "args.eval = True\n",
        "# LANGUAGE_CODES =['DE', 'GA', 'HI', 'PT', 'ZH']\n",
        "# if args.eval:\n",
        "for code in LANGUAGE_CODES:\n",
        "    print('#' * 20)\n",
        "    print('# Evaluating Language: {}'.format(code))\n",
        "    print('#' * 20)\n",
        "    test_iterator = SkorchBucketIterator(\n",
        "        dataset=SentenceDataset(data=(x_dev[code], y_dev[code])),\n",
        "        batch_size=1,\n",
        "        sort=False,\n",
        "        sort_key=lambda x: len(x.sentence),\n",
        "        shuffle=False,\n",
        "        train=False,\n",
        "        one_hot=ONE_HOT_OUTPUT,\n",
        "        device=DEVICE)\n",
        "    args.dev_file = '{}/{}/dev.cupt'.format(BASE_DIR, code)\n",
        "    evaluate_model(net, test_iterator, tokenizer, args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jX5Yc2I9rhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"#\" * 20)\n",
        "print(\"\\nTraining finished!!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7yFEHUG-ldw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}