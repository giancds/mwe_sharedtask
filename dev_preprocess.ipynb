{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 52380,\n",
      "  \"iopub_port\": 52381,\n",
      "  \"stdin_port\": 52382,\n",
      "  \"control_port\": 52384,\n",
      "  \"hb_port\": 52383,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"f966645b-dede72324c6b2277720cafb4\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-5febf5bf-4316-4b4b-bfbf-0ae8cdb8581f.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_type = 'bert-base-multilingual-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_tokenize_dataset(train_files, tokenizer, train=True):\n",
    "\n",
    "    if len(train_files) == 0:\n",
    "        files = []\n",
    "        for root, _, files in os.walk('data/'):\n",
    "            for _file in files:\n",
    "                if train:\n",
    "                    if _file == 'train.cupt':\n",
    "                        files.append(os.path.join(root, _file))\n",
    "                else:\n",
    "                    if _file == 'dev.cupt':\n",
    "                        files.append(os.path.join(root, _file))\n",
    "\n",
    "    else:\n",
    "        files = train_files\n",
    "\n",
    "    for _file in files:\n",
    "        print(_file)\n",
    "    cls = tokenizer.encode('[CLS]')[1]\n",
    "    sep = tokenizer.encode('[SEP]')[1]\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    for _file in files:\n",
    "        with open(_file) as text:\n",
    "            tmp_line = []\n",
    "            tmp_label = []\n",
    "            for line in text:\n",
    "                if line == '\\n':\n",
    "                    sentences.append([cls] + tmp_line + [sep])\n",
    "                    labels.append([0] + tmp_label + [0])\n",
    "                    tmp_line = []\n",
    "                    tmp_label = []\n",
    "                elif not line.startswith('#'):\n",
    "                    feats = line.split()\n",
    "                    if '-' not in feats[0]:\n",
    "                        _label = 0 if feats[10] == '*' else 1\n",
    "                        tokens = tokenizer.encode(feats[1])\n",
    "                        tokens = tokens[1:-1]\n",
    "                        _label = [_label] * len(tokens)\n",
    "                        tmp_line += tokens\n",
    "                        tmp_label += _label\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/DE/train.cupt\n",
      "data/DE/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'DE'\n",
    "train_files = ['data/{}/train.cupt'.format(code)]\n",
    "dev_files = ['data/{}/dev.cupt'.format(code)]\n",
    "\n",
    "x_train, y_train = load_and_tokenize_dataset(train_files, tokenizer, train=True)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(dev_files, tokenizer, train=False)\n",
    "\n",
    "data[code] = {\n",
    "    'x_train': x_train,\n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev,\n",
    "    'y_dev': y_dev,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/GA/train.cupt\n",
      "data/GA/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'GA'\n",
    "train_files = ['data/{}/train.cupt'.format(code)]\n",
    "dev_files = ['data/{}/dev.cupt'.format(code)]\n",
    "\n",
    "x_train, y_train = load_and_tokenize_dataset(train_files, tokenizer, train=True)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(dev_files, tokenizer, train=False)\n",
    "\n",
    "data[code] = {\n",
    "    'x_train': x_train,\n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev,\n",
    "    'y_dev': y_dev,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/HI/train.cupt\n",
      "data/HI/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'HI'\n",
    "train_files = ['data/{}/train.cupt'.format(code)]\n",
    "dev_files = ['data/{}/dev.cupt'.format(code)]\n",
    "\n",
    "x_train, y_train = load_and_tokenize_dataset(train_files, tokenizer, train=True)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(dev_files, tokenizer, train=False)\n",
    "\n",
    "data[code] = {\n",
    "    'x_train': x_train,\n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev,\n",
    "    'y_dev': y_dev,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/PT/train.cupt\n",
      "data/PT/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'PT'\n",
    "train_files = ['data/{}/train.cupt'.format(code)]\n",
    "dev_files = ['data/{}/dev.cupt'.format(code)]\n",
    "\n",
    "x_train, y_train = load_and_tokenize_dataset(train_files, tokenizer, train=True)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(dev_files, tokenizer, train=False)\n",
    "\n",
    "data[code] = {\n",
    "    'x_train': x_train,\n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev,\n",
    "    'y_dev': y_dev,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ZH/train.cupt\n",
      "data/ZH/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'ZH'\n",
    "train_files = ['data/{}/train.cupt'.format(code)]\n",
    "dev_files = ['data/{}/dev.cupt'.format(code)]\n",
    "\n",
    "x_train, y_train = load_and_tokenize_dataset(train_files, tokenizer, train=True)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(dev_files, tokenizer, train=False)\n",
    "\n",
    "data[code] = {\n",
    "    'x_train': x_train,\n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev,\n",
    "    'y_dev': y_dev,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/{}.tokenized.pkl'.format(bert_type), 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['PT']['x_train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
