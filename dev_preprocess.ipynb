{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_type = 'bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(bert_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = {\n",
    "    '*': 0,\n",
    "    'IAV': 1,\n",
    "    'IRV': 2,\n",
    "    'LVC.cause': 3,\n",
    "    'LVC.full': 4,\n",
    "    'LS.ICV': 5,\n",
    "    'MVC': 6,\n",
    "    'VID': 7,\n",
    "    'VPC.full': 8,\n",
    "    'VPC.semi': 9,\n",
    "#     '<unlabeled>': 9\n",
    "}\n",
    "len(features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'IAV_' in features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.get('VID', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1:LVC.full', '2:LVC.full']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1:LVC.full;2:LVC.full'.split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_labels(feature, current_codes):\n",
    "    feats = feature.split(';')\n",
    "    if len(feats) == 1:\n",
    "        feats = feats[0].split(':')\n",
    "        if len(feats) == 1:\n",
    "            label = current_codes.get(feats[0], features['*'])\n",
    "            label = features.get(label)\n",
    "        else:\n",
    "            label = features[feats[1]]\n",
    "            current_codes[feats[0]] = feats[1]\n",
    "    elif len(feats) > 1:\n",
    "        _feats = feats[0].split(':')\n",
    "        if len(_feats) == 1:\n",
    "            label = current_codes.get(_feats[0], features['*'])\n",
    "            label = features.get(label)\n",
    "        else:\n",
    "            label = features[_feats[1]]\n",
    "            current_codes[_feats[0]] = _feats[1]\n",
    "        for f in feats[1:]:\n",
    "            n = f.split(':')\n",
    "            if len(n) > 1:\n",
    "                current_codes[n[0]] = n[1]\n",
    "    return label, current_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, {'1': 'VID'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_extract_labels('1:VID', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, {'1': 'LVC.full', '2': 'LVC.full'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_extract_labels('1:LVC.full;2:LVC.full', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, {'1': 'LVC.full', '2': 'LVC.full'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_extract_labels('1;2:LVC.full', {'1': 'LVC.full'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, {'1': 'LVC.full'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_extract_labels('1:LVC.full', {'1': 'LVC.full'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, {'1': 'LVC.full'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_extract_labels('1;2', {'1': 'LVC.full'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, {'1': 'LVC.full', '2': 'LVC.full'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_extract_labels('2:LVC.full', {'1': 'LVC.full'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, {'1': 'LVC.full'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_extract_labels('1', {'1': 'LVC.full'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, {'1': 'LVC.full', '2': 'VID', '3': 'VID'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_extract_labels('2;3:VID', {'1': 'LVC.full', '2': 'VID'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, {'1': 'LVC.full', '2': 'VID'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_extract_labels('*', {'1': 'LVC.full', '2': 'VID'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_tokenize_dataset(train_files, tokenizer, train=True, binary=True):\n",
    "\n",
    "    if len(train_files) == 0:\n",
    "        files = []\n",
    "        for root, _, files in os.walk('data/'):\n",
    "            for _file in files:\n",
    "                if train:\n",
    "                    if _file == 'train.cupt':\n",
    "                        files.append(os.path.join(root, _file))\n",
    "                else:\n",
    "                    if _file == 'dev.cupt':\n",
    "                        files.append(os.path.join(root, _file))\n",
    "\n",
    "    else:\n",
    "        files = train_files\n",
    "\n",
    "    for _file in files:\n",
    "        print(_file)\n",
    "    cls = tokenizer.encode('[CLS]')[1]\n",
    "    sep = tokenizer.encode('[SEP]')[1]\n",
    "\n",
    "    sentences, labels = [], []\n",
    "    for _file in files:\n",
    "        with open(_file) as text:\n",
    "            tmp_line = []\n",
    "            tmp_label = []\n",
    "            current_codes = {}\n",
    "            for i, line in enumerate(text):\n",
    "#                 print(line)\n",
    "                if line == '\\n':\n",
    "                    sentences.append([cls] + tmp_line + [sep])\n",
    "                    labels.append([0] + tmp_label + [0])\n",
    "                    tmp_line = []\n",
    "                    tmp_label = []\n",
    "                    current_codes = {}\n",
    "                elif not line.startswith('#'):\n",
    "                    feats = line.replace('\\n', '').split('\\t')\n",
    "                    if not '-' in feats[0]:                        \n",
    "                        _label = [0] if feats[10] == '*' else ([1] if binary else _extract_labels(feats[10], current_codes))\n",
    "                        if len(_label) > 1:\n",
    "                            _label, current_codes = _label\n",
    "                        else:\n",
    "                            _label = _label[0]\n",
    "                        tokens = tokenizer.encode(feats[1])\n",
    "                        tokens = tokens[1:-1]\n",
    "                        _label = [_label] * len(tokens)\n",
    "                        tmp_line += tokens\n",
    "                        tmp_label += _label\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/DE/train.cupt\n",
      "data/DE/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'DE'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "de = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/EL/train.cupt\n",
      "data/EL/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'EL'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "el = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/EU/train.cupt\n",
      "data/EU/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'EU'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "eu = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/FR/train.cupt\n",
      "data/FR/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'FR'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "fr = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/GA/train.cupt\n",
      "data/GA/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'GA'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "ga = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/HE/train.cupt\n",
      "data/HE/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'HE'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "he = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/HI/train.cupt\n",
      "data/HI/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'HI'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "hi = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/IT/train.cupt\n",
      "data/IT/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'IT'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "it = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/PL/train.cupt\n",
      "data/PL/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'PL'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "pl = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/PT/train.cupt\n",
      "data/PT/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'PT'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "pt = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/RO/train.cupt\n",
      "data/RO/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'RO'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "ro = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/SV/train.cupt\n",
      "data/SV/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'SV'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "sv = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/TR/train.cupt\n",
      "data/TR/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'TR'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "tr = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ZH/train.cupt\n",
      "data/ZH/dev.cupt\n"
     ]
    }
   ],
   "source": [
    "code = 'ZH'\n",
    "x_train, y_train = load_and_tokenize_dataset(['data/{}/train.cupt'.format(code)], tokenizer, binary=False)\n",
    "x_dev, y_dev = load_and_tokenize_dataset(['data/{}/dev.cupt'.format(code)], tokenizer, binary=False)\n",
    "zh = {\n",
    "    'x_train': x_train, \n",
    "    'y_train': y_train,\n",
    "    'x_dev': x_dev, \n",
    "    'y_dev': y_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'DE': de,\n",
    "    'EL': el,\n",
    "    'EU': eu,\n",
    "    'FR': fr,    \n",
    "    'GA': ga,\n",
    "    'HE': he,\n",
    "    'HI': hi,\n",
    "    'IT': it,\n",
    "    'PL': pl,\n",
    "    'PT': pt,\n",
    "    'RO': ro,\n",
    "    'SV': sv,\n",
    "    'TR': tr,\n",
    "    'ZH': zh,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/{}.multilabel.tokenized.all.pkl'.format(bert_type), 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
