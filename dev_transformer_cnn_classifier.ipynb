{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 59971,\n",
      "  \"iopub_port\": 59972,\n",
      "  \"stdin_port\": 59973,\n",
      "  \"control_port\": 59975,\n",
      "  \"hb_port\": 59974,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"a17a4a3f-1c45f4d5ec91773a4dc84304\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-a58010ed-00f5-4dcf-9b50-c095da8d7c91.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = sys.argv[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from skorch.callbacks import ProgressBar, EarlyStopping, Checkpoint\n",
    "from skorch.helper import predefined_split\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from models import CNNClassifier\n",
    "from preprocess import load_tokenized_data\n",
    "from skorch_custom import SentenceDataset, SkorchBucketIterator\n",
    "from skorch_custom import IdiomClassifier, CustomScorer\n",
    "from evaluation import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Classifier using CNNs')\n",
    "parser.add_argument(\n",
    "    '--bert_type',\n",
    "    type=str,\n",
    "    default='distilbert-base-multilingual-cased',\n",
    "    help='transormer model [should be a miltilingual model]')\n",
    "parser.add_argument(\n",
    "    '--bert_device',\n",
    "    type=str,\n",
    "    default='gpu',\n",
    "    help='device to run the transformer model')\n",
    "parser.add_argument(\n",
    "    '--metric',\n",
    "    type=str,\n",
    "    default='f1',\n",
    "    help='sklearn metric to evaluate the model while training')\n",
    "parser.add_argument(\n",
    "    '--nfilters',\n",
    "    type=int,\n",
    "    default=128,\n",
    "    help='number of convolution filters')\n",
    "parser.add_argument(\n",
    "    '--kernels',\n",
    "    type=list,\n",
    "    default=[1, 2, 3, 4, 5],\n",
    "    help='number of convolution filters')\n",
    "parser.add_argument(\n",
    "    '--pool_stride',\n",
    "    type=int,\n",
    "    default=3,\n",
    "    help='size of the stride for the pooling operation')\n",
    "parser.add_argument(\n",
    "    '--dropout',\n",
    "    type=float,\n",
    "    default=0.2,\n",
    "    help='dropout probability for the dense layer')\n",
    "parser.add_argument(\n",
    "    '--output_activation',\n",
    "    type=str,\n",
    "    default='sigmoid',\n",
    "    help='output activation')\n",
    "parser.add_argument(\n",
    "    '--batch_size',\n",
    "    type=int,\n",
    "    default=32,\n",
    "    help='training batch size')\n",
    "parser.add_argument(\n",
    "    '--max_epochs',\n",
    "    type=int,\n",
    "    default=100,\n",
    "    help='max number of epochs to train the model')\n",
    "\n",
    "args = parser.parse_args()\n",
    "transformer_device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() and args.bert_device == 'gpu'\n",
    "    else 'cpu')\n",
    "ONE_HOT_OUTPUT = args.output_activation == 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')     # pylint: disable=no-member\n",
    "LANGUAGE_CODES = ['DE', 'GA', 'HI', 'PT', 'ZH']\n",
    "CWD = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_dev, y_dev) = load_tokenized_data(\n",
    "    datafile='{}/data/{}.tokenized.pkl'.format(CWD, args.bert_type),\n",
    "    language_codes=LANGUAGE_CODES,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.concatenate(y_train).reshape(-1)\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(targets),\n",
    "                                     y=targets).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.bert_type)\n",
    "transformer = AutoModel.from_pretrained(args.bert_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier(args, transformer, transformer_device)\n",
    "model.to(DEVICE)     # pylint: disable=no-member\n",
    "model.freeze_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = IdiomClassifier(\n",
    "    module=model,\n",
    "    class_weights=class_weights,\n",
    "     #\n",
    "    iterator_train=SkorchBucketIterator,\n",
    "    iterator_train__batch_size=args.batch_size,\n",
    "    iterator_train__sort_key=lambda x: len(x.sentence),\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__device=DEVICE,\n",
    "    iterator_train__one_hot=ONE_HOT_OUTPUT,\n",
    "     #\n",
    "    iterator_valid=SkorchBucketIterator,\n",
    "    iterator_valid__batch_size=32,\n",
    "    iterator_valid__sort_key=lambda x: len(x.sentence),\n",
    "    iterator_valid__shuffle=True,\n",
    "    iterator_valid__device=DEVICE,\n",
    "    iterator_valid__one_hot=ONE_HOT_OUTPUT,\n",
    "\n",
    "    train_split=predefined_split(SentenceDataset(data=(x_val[0:50], y_val[0:50]))),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=nn.BCELoss,\n",
    "    callbacks=[\n",
    "        ProgressBar(batches_per_epoch=len(x_train) // args.batch_size + 1),\n",
    "        CustomScorer(scoring=None, lower_is_better=False, use_caching=False),\n",
    "        EarlyStopping(monitor='score_best', patience=5),\n",
    "        Checkpoint(monitor='score_best')\n",
    "    ],\n",
    "    device=DEVICE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486232198c494baf88df14c50a737943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=706.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1270    0]\n",
      " [  85    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97      1270\n",
      "         1.0       0.00      0.00      0.00        85\n",
      "\n",
      "    accuracy                           0.94      1355\n",
      "   macro avg       0.47      0.50      0.48      1355\n",
      "weighted avg       0.88      0.94      0.91      1355\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gian/.virtualenvs/research/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    score    train_loss    valid_loss    cp     dur\n",
      "-------  -------  ------------  ------------  ----  ------\n",
      "      1   \u001b[36m0.0000\u001b[0m        \u001b[32m0.2833\u001b[0m        \u001b[35m4.3100\u001b[0m     +  4.1306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch_custom.IdiomClassifier'>[initialized](\n",
       "  module_=CNNClassifier(\n",
       "    (transformer): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (convolutions): ModuleList(\n",
       "      (0): Conv1d(768, 128, kernel_size=(1,), stride=(1,))\n",
       "      (1): Conv1d(768, 128, kernel_size=(2,), stride=(1,))\n",
       "      (2): Conv1d(768, 128, kernel_size=(3,), stride=(1,))\n",
       "      (3): Conv1d(768, 128, kernel_size=(4,), stride=(1,))\n",
       "      (4): Conv1d(768, 128, kernel_size=(5,), stride=(1,))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (fully_connected): Linear(in_features=210, out_features=1, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(SentenceDataset(data=(x_train[0:2], y_train[0:2])), y=None, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "# Evaluating Language: GA\n",
      "####################\n",
      "0\n",
      "## Global evaluation\n",
      "* MWE-based: P=102/497=0.2052 R=102/126=0.8095 F=0.3274\n",
      "* Tok-based: P=287/7024=0.0409 R=287/287=1.0000 F=0.0785\n",
      "\n",
      "## Per-category evaluation (partition of Global)\n",
      "* IAV: MWE-proportion: gold=42/126=33% pred=38/497=8%\n",
      "* IAV: MWE-based: P=33/38=0.8684 R=33/42=0.7857 F=0.8250\n",
      "* IAV: Tok-based: P=77/88=0.8750 R=77/86=0.8953 F=0.8851\n",
      "* LVC.cause: MWE-proportion: gold=22/126=17% pred=22/497=4%\n",
      "* LVC.cause: MWE-based: P=18/22=0.8182 R=18/22=0.8182 F=0.8182\n",
      "* LVC.cause: Tok-based: P=49/58=0.8448 R=49/49=1.0000 F=0.9159\n",
      "* LVC.full: MWE-proportion: gold=29/126=23% pred=27/497=5%\n",
      "* LVC.full: MWE-based: P=25/27=0.9259 R=25/29=0.8621 F=0.8929\n",
      "* LVC.full: Tok-based: P=61/66=0.9242 R=61/65=0.9385 F=0.9313\n",
      "* <unlabeled>: MWE-proportion: gold=0/126=0% pred=381/497=77%\n",
      "* <unlabeled>: MWE-based: P=0/381=0.0000 R=0/0=0.0000 F=0.0000\n",
      "* <unlabeled>: Tok-based: P=0/6733=0.0000 R=0/0=0.0000 F=0.0000\n",
      "* VID: MWE-proportion: gold=22/126=17% pred=18/497=4%\n",
      "* VID: MWE-based: P=16/18=0.8889 R=16/22=0.7273 F=0.8000\n",
      "* VID: Tok-based: P=54/56=0.9643 R=54/65=0.8308 F=0.8926\n",
      "* VPC.full: MWE-proportion: gold=6/126=5% pred=6/497=1%\n",
      "* VPC.full: MWE-based: P=6/6=1.0000 R=6/6=1.0000 F=1.0000\n",
      "* VPC.full: Tok-based: P=12/12=1.0000 R=12/12=1.0000 F=1.0000\n",
      "* VPC.semi: MWE-proportion: gold=5/126=4% pred=5/497=1%\n",
      "* VPC.semi: MWE-based: P=4/5=0.8000 R=4/5=0.8000 F=0.8000\n",
      "* VPC.semi: Tok-based: P=10/11=0.9091 R=10/10=1.0000 F=0.9524\n",
      "\n",
      "## MWE continuity (partition of Global)\n",
      "* Continuous: MWE-proportion: gold=59/126=47% pred=262/497=53%\n",
      "* Continuous: MWE-based: P=51/262=0.1947 R=51/59=0.8644 F=0.3178\n",
      "* Discontinuous: MWE-proportion: gold=67/126=53% pred=235/497=47%\n",
      "* Discontinuous: MWE-based: P=51/235=0.2170 R=51/67=0.7612 F=0.3377\n",
      "\n",
      "## Number of tokens (partition of Global)\n",
      "* Multi-token: MWE-proportion: gold=126/126=100% pred=472/497=95%\n",
      "* Multi-token: MWE-based: P=102/472=0.2161 R=102/126=0.8095 F=0.3411\n",
      "* Single-token: MWE-proportion: gold=0/126=0% pred=25/497=5%\n",
      "* Single-token: MWE-based: P=0/25=0.0000 R=0/0=0.0000 F=0.0000\n",
      "\n",
      "## Whether seen in train (partition of Global)\n",
      "* Seen-in-train: MWE-proportion: gold=26/126=21% pred=22/497=4%\n",
      "* Seen-in-train: MWE-based: P=20/22=0.9091 R=20/26=0.7692 F=0.8333\n",
      "* Unseen-in-train: MWE-proportion: gold=100/126=79% pred=475/497=96%\n",
      "* Unseen-in-train: MWE-based: P=82/475=0.1726 R=82/100=0.8200 F=0.2852\n",
      "\n",
      "## Whether identical to train (partition of Seen-in-train)\n",
      "* Variant-of-train: MWE-proportion: gold=21/26=81% pred=18/22=82%\n",
      "* Variant-of-train: MWE-based: P=16/18=0.8889 R=16/21=0.7619 F=0.8205\n",
      "* Identical-to-train: MWE-proportion: gold=5/26=19% pred=4/22=18%\n",
      "* Identical-to-train: MWE-based: P=4/4=1.0000 R=4/5=0.8000 F=0.8889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "code = 'GA'\n",
    "print('#' * 20)\n",
    "print('# Evaluating Language: {}'.format(code))\n",
    "print('#' * 20)\n",
    "test_iterator = SkorchBucketIterator(\n",
    "    dataset=SentenceDataset(data=(x_dev[code], y_dev[code])),\n",
    "    batch_size=32,\n",
    "    sort=False,\n",
    "    sort_key=lambda x: len(x.sentence),\n",
    "    shuffle=False,\n",
    "    train=False,\n",
    "    one_hot=args.output_activation == 'softmax',\n",
    "    device=DEVICE)\n",
    "args.dev_file = '{}/data/{}/dev.cupt'.format(CWD, code)\n",
    "evaluate_model(net, test_iterator, tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
