{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 60092,\n",
      "  \"iopub_port\": 60093,\n",
      "  \"stdin_port\": 60094,\n",
      "  \"control_port\": 60096,\n",
      "  \"hb_port\": 60095,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"18b120aa-4db4112b9911af4facceb00b\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-6bba0894-5bdd-4d88-8d57-540d7893233e.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = sys.argv[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from skorch.callbacks import ProgressBar, EarlyStopping, Checkpoint\n",
    "from skorch.helper import predefined_split\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from models import CNNClassifier\n",
    "from preprocess import load_tokenized_data\n",
    "from skorch_custom import SentenceDataset, SkorchBucketIterator\n",
    "# from skorch_custom import IdiomClassifier, CustomScorer\n",
    "from evaluation import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Classifier using CNNs')\n",
    "parser.add_argument(\n",
    "    '--bert_type',\n",
    "    type=str,\n",
    "    default='distilbert-base-multilingual-cased',\n",
    "    help='transormer model [should be a miltilingual model]')\n",
    "parser.add_argument(\n",
    "    '--bert_device',\n",
    "    type=str,\n",
    "    default='gpu',\n",
    "    help='device to run the transformer model')\n",
    "parser.add_argument(\n",
    "    '--metric',\n",
    "    type=str,\n",
    "    default='f1',\n",
    "    help='sklearn metric to evaluate the model while training')\n",
    "parser.add_argument(\n",
    "    '--nfilters',\n",
    "    type=int,\n",
    "    default=128,\n",
    "    help='number of convolution filters')\n",
    "parser.add_argument(\n",
    "    '--kernels',\n",
    "    type=list,\n",
    "    default=[1, 2, 3, 4, 5],\n",
    "    help='number of convolution filters')\n",
    "parser.add_argument(\n",
    "    '--pool_stride',\n",
    "    type=int,\n",
    "    default=3,\n",
    "    help='size of the stride for the pooling operation')\n",
    "parser.add_argument(\n",
    "    '--dropout',\n",
    "    type=float,\n",
    "    default=0.2,\n",
    "    help='dropout probability for the dense layer')\n",
    "parser.add_argument(\n",
    "    '--output_activation',\n",
    "    type=str,\n",
    "    default='sigmoid',\n",
    "    help='output activation')\n",
    "parser.add_argument(\n",
    "    '--batch_size',\n",
    "    type=int,\n",
    "    default=32,\n",
    "    help='training batch size')\n",
    "parser.add_argument(\n",
    "    '--max_epochs',\n",
    "    type=int,\n",
    "    default=100,\n",
    "    help='max number of epochs to train the model')\n",
    "\n",
    "args = parser.parse_args()\n",
    "transformer_device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() and args.bert_device == 'gpu'\n",
    "    else 'cpu')\n",
    "ONE_HOT_OUTPUT = args.output_activation == 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')     # pylint: disable=no-member\n",
    "LANGUAGE_CODES = ['DE', 'GA', 'HI', 'PT', 'ZH']\n",
    "CWD = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_dev, y_dev) = load_tokenized_data(\n",
    "    datafile='{}/data/{}.tokenized.pkl'.format(CWD, args.bert_type),\n",
    "    language_codes=LANGUAGE_CODES,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.concatenate(y_train).reshape(-1)\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(targets),\n",
    "                                     y=targets).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.bert_type)\n",
    "transformer = AutoModel.from_pretrained(args.bert_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier(args, transformer, transformer_device)\n",
    "model.to(DEVICE)     # pylint: disable=no-member\n",
    "model.freeze_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdiomClassifier(skorch.NeuralNetClassifier):\n",
    "\n",
    "    def __init__(self, print_report=True, class_weights=None, *args, **kwargs):\n",
    "        self.print_report = print_report\n",
    "        self.class_weights = class_weights\n",
    "        if class_weights is None:\n",
    "            self.class_weights = [1.0, 1.0]\n",
    "        super(IdiomClassifier, self).__init__(*args, **kwargs)\n",
    "        self.set_params(callbacks__valid_acc=None)\n",
    "        self.set_params(criterion__reduction='none')\n",
    "\n",
    "    def get_loss(self, y_pred, y_true, X, *args, **kwargs):\n",
    "        loss = super().get_loss(y_pred.view(-1), y_true.view(-1), X, *args,\n",
    "                                **kwargs)\n",
    "        weights = torch.ones_like(y_true) * y_true\n",
    "        weights = torch.where(\n",
    "            y_true == 0,\n",
    "            torch.tensor(self.class_weights[0]).float().to(self.device),\n",
    "            torch.where(y_true == 1,\n",
    "                        torch.tensor(self.class_weights[1]).to(self.device).float(),\n",
    "                        torch.tensor(-1.0).to(self.device)))\n",
    "        loss = (loss * weights.view(-1))\n",
    "        mask = (y_true >= 0).int()\n",
    "        loss = (loss * mask.view(-1)).mean()\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.module.eval()\n",
    "        y_pred = self.module(X)\n",
    "        if len(y_pred.shape) > 2:\n",
    "            y_pred = torch.argmax(y_pred, dim=2)\n",
    "        else:\n",
    "            y_pred = (y_pred > 0.5).int()\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        self.module.eval()\n",
    "        ds = self.get_dataset(X)\n",
    "        target_iterator = self.get_iterator(ds, training=False)\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for x, y in target_iterator:\n",
    "            preds = self.predict(x)\n",
    "            y_pred.append(preds.view(-1))\n",
    "            if len(y.shape) > 2:\n",
    "                y = torch.argmax(y, dim=2)\n",
    "            y_true.append(y.view(-1))\n",
    "        y_true = torch.cat(y_true).cpu().view(-1).detach().numpy().tolist()\n",
    "        y_pred = torch.cat(y_pred).cpu().view(-1).detach().numpy().tolist()\n",
    "\n",
    "        tt, tp = [], []\n",
    "        for t, p in zip(y_true, y_pred):\n",
    "            if t >= 0:\n",
    "                tt.append(t)\n",
    "                tp.append(p)\n",
    "\n",
    "        y_true = tt\n",
    "        y_pred = tp\n",
    "\n",
    "        if self.print_report:\n",
    "            print('Confusion matrix')\n",
    "            print(confusion_matrix(y_true, y_pred))\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        return f1_score(y_true, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScorer(skorch.callbacks.EpochScoring):\n",
    "\n",
    "    def on_epoch_end(self, net, dataset_train, dataset_valid, **kwargs):\n",
    "        current_score = net.score(dataset_valid)\n",
    "        self._record_score(net.history, current_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = IdiomClassifier(\n",
    "    module=model,\n",
    "    class_weights=class_weights,\n",
    "    print_report=False,\n",
    "     #\n",
    "    iterator_train=SkorchBucketIterator,\n",
    "    iterator_train__batch_size=args.batch_size,\n",
    "    iterator_train__sort_key=lambda x: len(x.sentence),\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__device=DEVICE,\n",
    "    iterator_train__one_hot=ONE_HOT_OUTPUT,\n",
    "     #\n",
    "    iterator_valid=SkorchBucketIterator,\n",
    "    iterator_valid__batch_size=32,\n",
    "    iterator_valid__sort_key=lambda x: len(x.sentence),\n",
    "    iterator_valid__shuffle=True,\n",
    "    iterator_valid__device=DEVICE,\n",
    "    iterator_valid__one_hot=ONE_HOT_OUTPUT,\n",
    "\n",
    "    train_split=predefined_split(SentenceDataset(data=(x_val[0:50], y_val[0:50]))),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=nn.BCELoss,\n",
    "    callbacks=[\n",
    "        ProgressBar(batches_per_epoch=len(x_train) // args.batch_size + 1),\n",
    "        CustomScorer(scoring=None, lower_is_better=False, use_caching=False),\n",
    "        EarlyStopping(monitor='score_best', patience=5),\n",
    "        Checkpoint(monitor='score_best')\n",
    "    ],\n",
    "    device=DEVICE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(SentenceDataset(data=(x_train[0:2], y_train[0:2])), y=None, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code = 'GA'\n",
    "print('#' * 20)\n",
    "print('# Evaluating Language: {}'.format(code))\n",
    "print('#' * 20)\n",
    "test_iterator = SkorchBucketIterator(\n",
    "    dataset=SentenceDataset(data=(x_dev[code], y_dev[code])),\n",
    "    batch_size=32,\n",
    "    sort=False,\n",
    "    sort_key=lambda x: len(x.sentence),\n",
    "    shuffle=False,\n",
    "    train=False,\n",
    "    one_hot=args.output_activation == 'softmax',\n",
    "    device=DEVICE)\n",
    "args.dev_file = '{}/data/{}/dev.cupt'.format(CWD, code)\n",
    "evaluate_model(net, test_iterator, tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.1710000000000005e-05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5171 * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
