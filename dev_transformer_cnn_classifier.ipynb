{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 52103,\n",
      "  \"iopub_port\": 52104,\n",
      "  \"stdin_port\": 52105,\n",
      "  \"control_port\": 52107,\n",
      "  \"hb_port\": 52106,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"10b7a7a2-a6da95b45baf85af0c1ff347\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-921fcdeb-1f60-4392-8dc5-95241c2f1ea8.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = sys.argv[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from evaluation import MetricsReporterCallback, evaluate\n",
    "from utils import build_model_name, convert_flags_to_dict, define_cnn_flags\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from hyperopt import hp\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BASE_DIR = os.path.expanduser(\"~\")     # this will point to the user's home\n",
    "TRAIN_DIR = \"ray_results\"\n",
    "\n",
    "FLAGS = define_cnn_flags(tf.compat.v1.flags, BASE_DIR, TRAIN_DIR)\n",
    "FLAGS.conv_layers = [int(i) for i in FLAGS.conv_layers]\n",
    "FLAGS.dense_layers = [int(i) for i in FLAGS.dense_layers]\n",
    "\n",
    "_config = convert_flags_to_dict(FLAGS)\n",
    "_config[\"codes\"] = (['DE', 'GA', 'HI', 'PT', 'ZH']\n",
    "                    if FLAGS.language_code is 'all' else [FLAGS.language_code])\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "RESULTS = {}\n",
    "\n",
    "# _config['tune'] = False\n",
    "\n",
    "_config['gpus'] = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(_config['gpus']) > 0:\n",
    "    tf.config.experimental.set_memory_growth(_config['gpus'][0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "\n",
    "    model_name = build_model_name(config)\n",
    "\n",
    "    with open('{}/data/{}.tokenized.pkl'.format(cwd, config[\"bert_type\"]),\n",
    "              'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(data.keys())\n",
    "    x_train, y_train, x_dev, y_dev = [], [], [], []\n",
    "    for code in config[\"codes\"]:\n",
    "        x_train += data[code][\"x_train\"]\n",
    "        y_train += data[code][\"y_train\"]\n",
    "\n",
    "        x_dev += data[code][\"x_dev\"]\n",
    "        y_dev += data[code][\"y_dev\"]\n",
    "\n",
    "    del data\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train)\n",
    "    max_len = x_train.shape[1]\n",
    "    y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train, maxlen=max_len)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train,\n",
    "                                                      y_train,\n",
    "                                                      test_size=0.15,\n",
    "                                                      random_state=SEED)\n",
    "\n",
    "    print(x_train.shape, x_train.shape, y_train.shape)\n",
    "    x_train = [x_train, (x_train > 0).astype(int)]\n",
    "    x_val = [x_val, (x_val > 0).astype(int)]\n",
    "\n",
    "\n",
    "\n",
    "    x_dev = tf.keras.preprocessing.sequence.pad_sequences(x_dev, maxlen=max_len)\n",
    "    x_dev = [x_dev, (x_dev > 0).astype(int)]\n",
    "\n",
    "    seq_lens = [len(seq) for seq in y_dev]\n",
    "    y_dev = tf.keras.preprocessing.sequence.pad_sequences(y_dev, maxlen=max_len)\n",
    "    print(x_dev[0].shape, x_dev[1].shape, y_dev.shape)\n",
    "\n",
    "    conv_config = ([config[\"conv_size\"]] * config[\"nconv\"]\n",
    "                   if config[\"nconv\"] > 0 and config[\"conv_size\"] > 0\n",
    "                   else config[\"conv_layers\"])\n",
    "\n",
    "    dense_config = ([config[\"dense_size\"]] * config[\"ndense\"]\n",
    "                    if config[\"ndense\"] > 0 and config[\"dense_size\"] > 0\n",
    "                    else config[\"dense_layers\"])\n",
    "\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "\n",
    "    with tf.device('/CPU:0'):\n",
    "        # embedding\n",
    "        transformer = TFAutoModel.from_pretrained(config[\"bert_type\"])\n",
    "        transformer.trainable = False\n",
    "        \n",
    "#         for param in transformer.weights:\n",
    "#             param.trainable = False\n",
    "\n",
    "        out = transformer(\n",
    "            input_ids, attention_mask=attention_mask, training=False)[0]\n",
    "\n",
    "    device = '/CPU:0'\n",
    "    if len(config['gpus']) > 0:\n",
    "        device = '/GPU:0'\n",
    "\n",
    "    with tf.device(device):\n",
    "\n",
    "        if config[\"linear_layer\"] > 0:\n",
    "            out = tf.keras.layers.Dense(\n",
    "                config[\"linear_layer\"], activation=None)(out)\n",
    "\n",
    "        for i, layer_size in enumerate(conv_config):\n",
    "            if i == 0:\n",
    "                out = tf.keras.layers.Conv1D(\n",
    "                    layer_size,\n",
    "                    config[\"strides\"],\n",
    "                    padding='same',\n",
    "                    activation=config[\"conv_activation\"],\n",
    "                    strides=1,\n",
    "                    input_shape=(None, x_train[0].shape[0], x_train[0].shape[1]))(out)\n",
    "            else:\n",
    "                out = tf.keras.layers.Conv1D(\n",
    "                    layer_size,\n",
    "                    config[\"strides\"],\n",
    "                    padding='same',\n",
    "                    activation=config[\"conv_activation\"],\n",
    "                    strides=1)(out)\n",
    "            out = tf.keras.layers.Dropout(config[\"conv_dropout\"])(out)\n",
    "\n",
    "        # Dense layers\n",
    "        for i, layer_size in enumerate(dense_config):\n",
    "            out = tf.keras.layers.Dense(\n",
    "                layer_size, activation=config[\"dense_activation\"])(out)\n",
    "            out = tf.keras.layers.Dropout(config[\"dense_dropout\"])(out)\n",
    "\n",
    "        if config[\"output_size\"] == 1:\n",
    "            out = tf.keras.layers.Dense(1, activation='sigmoid')(out)\n",
    "        else:\n",
    "            out = tf.keras.layers.Dense(\n",
    "                2, activation=config[\"output_activation\"])(out)\n",
    "\n",
    "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=out)\n",
    "\n",
    "        if config[\"optimizer\"] == 'adam':\n",
    "            optimizer = tf.keras.optimizers.Adam\n",
    "        elif config[\"optimizer\"] == 'rmsprop':\n",
    "            optimizer = tf.keras.optimizers.RMSprop\n",
    "        else:\n",
    "            optimizer = tf.keras.optimizers.SGD\n",
    "\n",
    "        # compiling model\n",
    "        model.compile(loss=config[\"loss_function\"],\n",
    "                    optimizer=optimizer(learning_rate=config[\"learning_rate\"],\n",
    "                                        clipnorm=config[\"clipnorm\"]),\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "#         print(model.summary())\n",
    "\n",
    "        # do this check again vecause we need y_train to be 1-D for class weights\n",
    "        if config[\"output_size\"] > 1:\n",
    "            y_train = tf.keras.utils.to_categorical(y_train)\n",
    "            y_val = tf.keras.utils.to_categorical(y_val)\n",
    "\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(config[\"train_dir\"] +\n",
    "                                                        model_name,\n",
    "                                                        save_best_only=True)\n",
    "        callbacks = [checkpoint]\n",
    "\n",
    "        if config[\"tune\"]:\n",
    "            callbacks.append(\n",
    "                MetricsReporterCallback(custom_validation_data=(x_val, y_val)))\n",
    "\n",
    "        if config[\"early_stop_patience\"] > 0:\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='loss',\n",
    "                min_delta=config[\"early_stop_delta\"],\n",
    "                patience=config[\"early_stop_patience\"])\n",
    "            callbacks.append(early_stop)\n",
    "\n",
    "        if config[\"log_tensorboard\"]:\n",
    "            tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=config[\"train_dir\"] + '/logs')\n",
    "            callbacks.append(tensorboard)\n",
    "\n",
    "        def lr_scheduler(epoch, lr):     # pylint: disable=C0103\n",
    "            lr_decay = config[\"lr_decay\"]**max(epoch - config[\"start_decay\"], 0.0)\n",
    "            return lr * lr_decay\n",
    "\n",
    "        if config[\"start_decay\"] > 0:\n",
    "            lrate = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "            callbacks.append(lrate)\n",
    "\n",
    "        print(model.summary())\n",
    "\n",
    "#         print('Train...')\n",
    "#         model.fit(x_train,\n",
    "#                 y_train,\n",
    "#                 batch_size=config[\"batch_size\"],\n",
    "#                 epochs=config[\"max_epochs\"],\n",
    "#                 callbacks=callbacks,\n",
    "#                 verbose=2,\n",
    "#                 validation_data=(x_val, y_val))\n",
    "\n",
    "#         # #####\n",
    "#         # Evaluation time\n",
    "#         #\n",
    "#         _results, y_pred = evaluate(\n",
    "#             model,\n",
    "#             test_data=(x_dev, y_dev),\n",
    "#             perword=True,\n",
    "#             seq_lens=seq_lens,\n",
    "#             output_dict=True)\n",
    "\n",
    "\n",
    "#     logs = {\n",
    "#         \"dev_label0_support\" :_results[\"0\"][\"support\"],\n",
    "#         \"dev_precision\" :_results[\"1\"][\"precision\"],\n",
    "#         \"dev_recall\" :_results[\"1\"][\"recall\"],\n",
    "#         \"dev_f1_score\" :_results[\"1\"][\"f1-score\"],\n",
    "#         \"dev_label1_support\" :_results[\"1\"][\"support\"],\n",
    "#         \"dev_macro_precision\" :_results[\"macro avg\"][\"precision\"],\n",
    "#         \"dev_macro_recall\" :_results[\"macro avg\"][\"recall\"],\n",
    "#         \"dev_macro_f1_score\" :_results[\"macro avg\"][\"f1-score\"],\n",
    "#         \"dev_weighted_precision\" :_results[\"weighted avg\"][\"precision\"],\n",
    "#         \"dev_weighted_recall\" :_results[\"weighted avg\"][\"recall\"],\n",
    "#         \"dev_weighted_f1_score\" :_results[\"weighted avg\"][\"f1-score\"]}\n",
    "\n",
    "#     if config[\"tune\"]:\n",
    "#         trial_id = ray.tune.track.trial_id()\n",
    "#     else:\n",
    "#         trial_id = 1\n",
    "\n",
    "\n",
    "#     RESULTS[str(trial_id)] = logs\n",
    "\n",
    "#     output_count = -1\n",
    "#     y_pred = np.array(y_pred).reshape(-1,).tolist()\n",
    "#     for code in config[\"codes\"]:\n",
    "#         with open('{}/data/{}/dev.cupt'.format(cwd, code), 'r') as dev:\n",
    "#             with open('{}/data/{}/dev.cupt'.format(cwd, code)) as test:\n",
    "#                 for line in dev:\n",
    "#                     if not line.startswith('#') and line is not '\\n':\n",
    "#                         test.write(line.replace('*', y_pred[output_count]))\n",
    "#                         output_count += 1\n",
    "#                     else:\n",
    "#                         test.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"dropout\":\n",
    "        hp.uniform(\"dropout\", 0.0, 0.9),\n",
    "    \"max_epochs\":\n",
    "        hp.choice(\"max_epochs\", [10, 20, 30, 50]),\n",
    "    \"early_stop_delta\":\n",
    "        hp.choice(\"early_stop_delta\", [0.001, 0.0001]),\n",
    "    \"early_stop_patience\":\n",
    "        hp.choice(\"early_stop_patience\", [10, 20]),\n",
    "    \"hidden_activation\":\n",
    "        hp.choice(\"hidden_activation\", [\"tanh', 'relu', 'elu', 'selu\"]),\n",
    "    \"output_activation\":\n",
    "        hp.choice(\"output_activation\", [\"sigmoid', 'softmax\"]),\n",
    "    \"clipnorm\":\n",
    "        hp.choice(\"clipnorm\", [0.5, 1.0, 2.5, 5.0, 10.0]),\n",
    "    \"learning_rate\":\n",
    "        hp.loguniform(\"learning_rate\", np.log(1e-4), np.log(1e-0)),\n",
    "    \"batch_size\":\n",
    "        hp.choice(\"batch_size\", [20, 24, 32, 64, 128]),\n",
    "    \"nlayers\":\n",
    "        hp.randint('nlayers', 1, 5) * 1,\n",
    "    \"layer_size\":\n",
    "        hp.randint('layer_size', 1, 101) * 10,\n",
    "}\n",
    "\n",
    "_config.update({\n",
    "    \"hidden_activation\": 'relu',\n",
    "    \"optimizer\": 'adam',\n",
    "    \"threads\": 4,\n",
    "    \"output_size\": 2,\n",
    "    \"num_samples\": 500\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model name nn_classifier.distilbert-base-multilingual-cased.all.100epochs.10-0.0010eStop.1-128-sigmoidlayers1strides.-0.100dropout2-sigmoid.output.0.5.thresh.weighted.binary_crossentropyLoss.32batch.adam.0.0001lr.0.8696-0decay.5.00norm.ckpt\n",
      "\n",
      "dict_keys(['DE', 'GA', 'HI', 'PT', 'ZH'])\n",
      "(56387, 338) (56387, 338) (56387, 338)\n",
      "(4330, 338) (4330, 338) (4330, 338)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 338)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 338)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_5 (TFDisti ((None, 338, 768),)  134734080   input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 338, 100)     76900       tf_distil_bert_model_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 338, 128)     12928       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_116 (Dropout)           (None, 338, 128)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 338, 10)      1290        dropout_116[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_117 (Dropout)           (None, 338, 10)      0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 338, 2)       22          dropout_117[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 134,825,220\n",
      "Trainable params: 91,140\n",
      "Non-trainable params: 134,734,080\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_model(_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TFAutoModel.from_pretrained(_config[\"bert_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.layers[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  134734080 \n",
      "=================================================================\n",
      "Total params: 134,734,080\n",
      "Trainable params: 0\n",
      "Non-trainable params: 134,734,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  134734080 \n",
      "=================================================================\n",
      "Total params: 134,734,080\n",
      "Trainable params: 134,734,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  134734080 \n",
      "=================================================================\n",
      "Total params: 134,734,080\n",
      "Trainable params: 0\n",
      "Non-trainable params: 134,734,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
