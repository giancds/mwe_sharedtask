{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible models\n",
    "\n",
    "`bert-base-multilingual-cased`: (New, recommended) 12-layer, 768-hidden, 12-heads, 110M parameters. Trained on cased text in the top 104 languages with the largest Wikipedias\n",
    "\n",
    "`xlm-mlm-100-1280`: 16-layer, 1280-hidden, 16-heads XLM model trained with MLM (Masked Language Modeling) on 100 languages.\n",
    "\n",
    "`distilbert-base-multilingual-cased`: 6-layer, 768-hidden, 12-heads, 134M parameters The multilingual DistilBERT model distilled from the Multilingual BERT model bert-base-multilingual-cased checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = sys.argv[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from evaluation import  evaluate #, MetricsReporterCallback,\n",
    "from utils import build_model_name, convert_flags_to_dict, define_nn_flags\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "# from ray.tune.integration.keras import TuneReporterCallback\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest import Repeater\n",
    "from hyperopt import hp\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BASE_DIR = os.path.expanduser(\"~\")     # this will point to the user's home\n",
    "TRAIN_DIR = \"ray_results\"\n",
    "\n",
    "FLAGS = define_nn_flags(tf.compat.v1.flags, BASE_DIR, TRAIN_DIR)\n",
    "FLAGS.layers = [int(i) for i in FLAGS.layers]\n",
    "\n",
    "_config = convert_flags_to_dict(FLAGS)\n",
    "_config[\"codes\"] = (['DE', 'GA', 'HI', 'PT', 'ZH']\n",
    "                    if FLAGS.language_code is 'all' else [FLAGS.language_code])\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(config):\n",
    "\n",
    "    model_name = build_model_name(config)\n",
    "\n",
    "    with open('{}/data/{}.embdata.pkl'.format(cwd, config[\"bert_type\"]),\n",
    "              'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    x_train = np.concatenate(\n",
    "        [data[code]['x_train'] for code in _config[\"codes\"]], axis=0)\n",
    "    y_train = np.concatenate(\n",
    "        [data[code]['y_train'] for code in _config[\"codes\"]], axis=0)\n",
    "    print(x_train.shape, y_train.shape)\n",
    "\n",
    "    x_dev = np.concatenate([data[code]['x_dev'] for code in _config[\"codes\"]],\n",
    "                           axis=0)\n",
    "    y_dev = np.concatenate([data[code]['y_dev'] for code in _config[\"codes\"]],\n",
    "                           axis=0)\n",
    "    print(x_dev.shape, y_dev.shape)\n",
    "\n",
    "    del data\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train,\n",
    "                                                      y_train,\n",
    "                                                      test_size=0.15,\n",
    "                                                      random_state=SEED)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    layer_config = ([config[\"layer_size\"]] * config[\"nlayers\"]\n",
    "                    if config[\"nlayers\"] > 0 and config[\"layer_size\"] > 0\n",
    "                    else config[\"layers\"])\n",
    "\n",
    "    # Dense layers\n",
    "    for i, layer_size in enumerate(layer_config):\n",
    "        if i == 0:\n",
    "            dense_layer = tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                input_shape=(x_train.shape[-1],),\n",
    "                activation=config[\"hidden_activation\"])\n",
    "        else:\n",
    "            dense_layer = tf.keras.layers.Dense(\n",
    "                layer_size, activation=config[\"hidden_activation\"])\n",
    "        model.add(dense_layer)\n",
    "        model.add(tf.keras.layers.Dropout(config[\"dropout\"]))\n",
    "\n",
    "    if config[\"output_size\"] == 1:\n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(\n",
    "                2,\n",
    "                activation=config[\"output_activation\"],\n",
    "            ))\n",
    "\n",
    "    if config[\"optimizer\"] == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam\n",
    "    elif config[\"optimizer\"] == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD\n",
    "\n",
    "    # compiling model\n",
    "    model.compile(loss=config[\"loss_function\"],\n",
    "                  optimizer=optimizer(learning_rate=config[\"learning_rate\"],\n",
    "                                      clipnorm=config[\"clipnorm\"]),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    class_weights = None\n",
    "    if config[\"weighted_loss\"]:\n",
    "        weights = class_weight.compute_class_weight('balanced',\n",
    "                                                    np.unique(y_train),\n",
    "                                                    y_train.reshape(-1))\n",
    "        class_weights = {}\n",
    "\n",
    "        for i in range(weights.shape[0]):\n",
    "            class_weights[i] = weights[i]\n",
    "\n",
    "    print('Class weights: {}'.format(class_weights))\n",
    "\n",
    "    # do this check again vecause we need y_train to be 1-D for class weights\n",
    "    if config[\"output_size\"] > 1:\n",
    "        y_train = tf.keras.utils.to_categorical(y_train)\n",
    "        y_val = tf.keras.utils.to_categorical(y_val)\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(config[\"train_dir\"] +\n",
    "                                                    model_name,\n",
    "                                                    save_best_only=True)\n",
    "    callbacks = [checkpoint]\n",
    "\n",
    "    if config[\"tune\"]:\n",
    "        callbacks.append(\n",
    "            MetricsReporterCallback(custom_validation_data=(x_val, y_val)))\n",
    "\n",
    "    if config[\"early_stop_patience\"] > 0:\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='loss',\n",
    "            min_delta=config[\"early_stop_delta\"],\n",
    "            patience=config[\"early_stop_patience\"])\n",
    "        callbacks.append(early_stop)\n",
    "\n",
    "    if config[\"log_tensorboard\"]:\n",
    "        tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=config[\"train_dir\"] + '/logs')\n",
    "        callbacks.append(tensorboard)\n",
    "\n",
    "    def lr_scheduler(epoch, lr):     # pylint: disable=C0103\n",
    "        lr_decay = config[\"lr_decay\"]**max(epoch - config[\"start_decay\"], 0.0)\n",
    "        return lr * lr_decay\n",
    "\n",
    "    if config[\"start_decay\"] > 0:\n",
    "        lrate = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "        callbacks.append(lrate)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    print('Train...')\n",
    "    model.fit(x_train,\n",
    "              y_train,\n",
    "              class_weight=class_weights,\n",
    "              batch_size=config[\"batch_size\"],\n",
    "              epochs=config[\"max_epochs\"],\n",
    "              callbacks=callbacks,\n",
    "              verbose=2,\n",
    "              validation_data=(x_val, y_val))\n",
    "\n",
    "    # #####\n",
    "    # Evaluation time\n",
    "    #\n",
    "    evaluate(model, test_data=(x_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"dropout\": hp.uniform(\"dropout\", 0.0, 0.9),\n",
    "    \"max_epochs\": hp.choice(\"max_epochs\", [10, 20, 30, 50]),\n",
    "    \"early_stop_delta\": hp.choice(\"early_stop_delta\", [0.001, 0.0001]),\n",
    "    \"early_stop_patience\": hp.choice(\"early_stop_patience\", [10, 20]),\n",
    "    \"hidden_activation\": hp.choice(\"hidden_activation\", ['tanh', 'relu', 'elu', 'selu']),\n",
    "    \"output_activation\": hp.choice(\"output_activation\", ['sigmoid', 'softmax']),\n",
    "    \"clipnorm\": hp.choice(\"clipnorm\", [0.5, 1.0, 2.5, 5.0, 10.0]),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(1e-4), np.log(1e-0)),\n",
    "    \"batch_size\": hp.choice(\"batch_size\", [20, 24, 32, 64, 128]),\n",
    "    \"nlayers\": hp.randint('nlayers', 1, 5) * 1,\n",
    "    \"layer_size\": hp.randint('layer_size', 1, 101) * 10,\n",
    "}\n",
    "\n",
    "_config.update({\n",
    "    \"hidden_activation\": 'relu',\n",
    "    \"optimizer\": 'adam',\n",
    "    \"threads\": 1,\n",
    "    \"output_size\": 2,\n",
    "    \"num_samples\": 500,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MetricsReporterCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Tune Callback for Keras.\"\"\"\n",
    "\n",
    "    def __init__(self, reporter=None, freq=\"epoch\", logs=None, custom_validation_data=None):\n",
    "        \"\"\"Initializer.\n",
    "\n",
    "        Args:\n",
    "            freq (str): Sets the frequency of reporting intermediate results.\n",
    "                One of [\"batch\", \"epoch\"].\n",
    "        \"\"\"\n",
    "        assert custom_validation_data, \"validation_data should not be None\"\n",
    "        self.custom_validation_data = custom_validation_data\n",
    "        self.iteration = 0\n",
    "        logs = logs or {}\n",
    "        # if freq not in [\"batch\", \"epoch\"]:\n",
    "        #     raise ValueError(\"{} not supported as a frequency.\".format(freq))\n",
    "        self.freq = \"epoch\"\n",
    "        super(MetricsReporterCallback, self).__init__()\n",
    "        self._results = None\n",
    "        self._batch_count = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # from ray import tune\n",
    "        # logs = logs or {}\n",
    "\n",
    "        logs = self._update_logs(logs, predict=self.iteration == 0)\n",
    "\n",
    "        if not self.freq == \"batch\":\n",
    "            return\n",
    "        self.iteration += 1\n",
    "        for metric in list(logs):\n",
    "            if \"loss\" in metric and \"neg_\" not in metric:\n",
    "                logs[\"neg_\" + metric] = -logs[metric]\n",
    "        if \"acc\" in logs:\n",
    "            tune.report(keras_info=logs, mean_accuracy=logs[\"acc\"])\n",
    "        else:\n",
    "            tune.report(keras_info=logs, mean_accuracy=logs.get(\"accuracy\"))\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "\n",
    "        print('Updating metrics')\n",
    "        val_predict = (np.asarray(\n",
    "            self.model.predict(self.custom_validation_data[0]))).round()\n",
    "\n",
    "        self._results = classification_report(\n",
    "            self.custom_validation_data[1], val_predict, output_dict=True)\n",
    "\n",
    "        logs = self._update_logs(logs or {})\n",
    "\n",
    "        if not self.freq == \"epoch\":\n",
    "            return\n",
    "        self.iteration += 1\n",
    "        for metric in list(logs):\n",
    "            if \"loss\" in metric and \"neg_\" not in metric:\n",
    "                logs[\"neg_\" + metric] = -logs[metric]\n",
    "        if \"acc\" in logs:\n",
    "            tune.track.log(keras_info=logs, mean_accuracy=logs[\"acc\"])\n",
    "        else:\n",
    "            tune.track.log(keras_info=logs, mean_accuracy=logs.get(\"accuracy\"))\n",
    "\n",
    "    def _update_logs(self, logs, predict=True):\n",
    "\n",
    "        if self._results is None:\n",
    "\n",
    "            logs.update({\n",
    "                # \"accuracy\": 0.0accuracy\"],\n",
    "                \"label0_precision\": 0.00,\n",
    "                \"label0_recall\": 0.00,\n",
    "                \"label0_f1_score\": 0.00,\n",
    "                \"label0_support\": 0.00,\n",
    "                \"label1_precision\": 0.00,\n",
    "                \"label1_recall\": 0.0,\n",
    "                \"label1_f1_score\": 0.0,\n",
    "                \"f1_score\": 0.0,\n",
    "                \"label1_support\": 0.0,\n",
    "                \"macro_precision\": 0.0,\n",
    "                \"macro_recall\": 0.0,\n",
    "                \"macro_f1_score\": 0.0,\n",
    "                \"macro_support\": 0.0,\n",
    "                \"weighted_precision\": 0.0,\n",
    "                \"weighted_recall\": 0.0,\n",
    "                \"weighted_f1_score\": 0.0,\n",
    "                \"weighted_support\": 0.0})\n",
    "        else:\n",
    "\n",
    "            logs.update({\n",
    "                # \"accuracy\" :self._results[\"accuracy\"],\n",
    "                \"label0_precision\" :self._results[\"0\"][\"precision\"],\n",
    "                \"label0_recall\" :self._results[\"0\"][\"recall\"],\n",
    "                \"label0_f1_score\" :self._results[\"0\"][\"f1-score\"],\n",
    "                \"label0_support\" :self._results[\"0\"][\"support\"],\n",
    "                \"label1_precision\" :self._results[\"1\"][\"precision\"],\n",
    "                \"label1_recall\" :self._results[\"1\"][\"recall\"],\n",
    "                \"label1_f1_score\" :self._results[\"1\"][\"f1-score\"],\n",
    "                \"f1_score\" :self._results[\"1\"][\"f1-score\"],\n",
    "                \"label1_support\" :self._results[\"1\"][\"support\"],\n",
    "                \"macro_precision\" :self._results[\"macro avg\"][\"precision\"],\n",
    "                \"macro_recall\" :self._results[\"macro avg\"][\"recall\"],\n",
    "                \"macro_f1_score\" :self._results[\"macro avg\"][\"f1-score\"],\n",
    "                \"macro_support\" :self._results[\"macro avg\"][\"support\"],\n",
    "                \"weighted_precision\" :self._results[\"weighted avg\"][\"precision\"],\n",
    "                \"weighted_recall\" :self._results[\"weighted avg\"][\"recall\"],\n",
    "                \"weighted_f1_score\" :self._results[\"weighted avg\"][\"f1-score\"],\n",
    "                \"weighted_support\" :self._results[\"weighted avg\"][\"support\"]})\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reporter = tune.CLIReporter()\n",
    "# reporter = tune.JupyterNotebookReporter(False)\n",
    "# reporter.add_metric_column('keras_info/label1_f1_score', 'f1-score')\n",
    "\n",
    "ray.shutdown()     # Restart Ray defensively in case the ray connection is lost.\n",
    "ray.init(num_cpus=2)\n",
    "results = tune.run(\n",
    "    train_model,\n",
    "    name=\"tune-nn-bert-classifier\",\n",
    "    config=_config,\n",
    "    stop={\n",
    "        \"keras_info/f1_score\": 0.99,\n",
    "        \"training_iteration\": 10**8\n",
    "    },\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 1,\n",
    "        \"gpu\": 0\n",
    "    },\n",
    "    num_samples=_config[\"num_samples\"],\n",
    "    checkpoint_freq=0,\n",
    "    checkpoint_at_end=False,\n",
    "    scheduler=AsyncHyperBandScheduler(\n",
    "        time_attr='epoch',\n",
    "        metric='f1_score',\n",
    "        mode='max',\n",
    "        max_t=400,\n",
    "        grace_period=20),\n",
    "    search_alg=HyperOptSearch(\n",
    "        search_space,\n",
    "        metric=\"keras_info/f1_score\",\n",
    "        mode=\"max\",\n",
    "        random_state_seed=SEED,\n",
    "        points_to_evaluate=[{\n",
    "            \"dropout\": 0.2,\n",
    "            \"max_epochs\": 2,\n",
    "            \"early_stop_delta\": 0,\n",
    "            \"early_stop_patience\": 0,\n",
    "            \"hidden_activation\": 1,\n",
    "            \"output_activation\": 0,\n",
    "            \"clipnorm\": 3,\n",
    "            \"learning_rate\": 0.0001,\n",
    "            \"batch_size\": 3,\n",
    "            \"nlayers\": 2,\n",
    "            \"layer_size\": 100            \n",
    "        }]\n",
    "    ),\n",
    "    progress_reporter=reporter,\n",
    "    verbose=1)\n",
    "results.dataframe().to_csv(\n",
    "    '{0}/nn_results{1}layers.csv'.format(\n",
    "        _config[\"train_dir\"], _config['layers']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_space = {\n",
    "    \"dropout\":\n",
    "        hp.uniform(\"dropout\", 0.0, 0.9),\n",
    "    \"max_epochs\":\n",
    "        hp.choice(\"max_epochs\", [10, 20, 30, 50]),\n",
    "    \"early_stop_delta\":\n",
    "        hp.choice(\"early_stop_delta\", [0.001, 0.0001]),\n",
    "    \"early_stop_patience\":\n",
    "        hp.choice(\"early_stop_patience\", [10, 20]),\n",
    "    \"hidden_activation\":\n",
    "        hp.choice(\"hidden_activation\", ['tanh', 'relu', 'elu', 'selu']),\n",
    "    \"output_activation\":\n",
    "        hp.choice(\"output_activation\", ['sigmoid', 'softmax']),\n",
    "    \"clipnorm\":\n",
    "        hp.choice(\"clipnorm\", [0.5, 1.0, 2.5, 5.0, 10.0]),\n",
    "    \"learning_rate\":\n",
    "        hp.loguniform(\"learning_rate\", np.log(1e-4), np.log(1e-0)),\n",
    "    \"batch_size\":\n",
    "        hp.choice(\"batch_size\", [20, 24, 32, 64, 128]),\n",
    "    \"nlayers\":\n",
    "        hp.randint('nlayers', 1, 5) * 1,\n",
    "    \"layer_size\":\n",
    "        hp.randint('layer_size', 1, 100) * 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
