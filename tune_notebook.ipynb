{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = sys.argv[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from preprocess import Features, load_dataset, pre_process_data\n",
    "from evaluation import MetricsReporterCallback, evaluate\n",
    "from utils import get_callbacks, get_optimizer\n",
    "from utils import define_cnn_flags, build_cnn_name, convert_flags_to_dict\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.integration.keras import TuneReporterCallback\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest import Repeater\n",
    "# from ray.tune.integration.keras import TuneReporterCallback\n",
    "from hyperopt import hp\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "#\n",
    "SEED = 42\n",
    "BASE_DIR = os.path.expanduser(\"~\")     # this will point to the user's home\n",
    "TRAIN_DIR = \"train_mwe_classifier\"\n",
    "\n",
    "# #####\n",
    "# Some hyperparameter definitions\n",
    "#\n",
    "FLAGS = define_cnn_flags(tf.compat.v1.flags, BASE_DIR, TRAIN_DIR)\n",
    "\n",
    "# define which feature we can use to train de model\n",
    "\n",
    "FLAGS.filters = [int(i) for i in FLAGS.filters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####\n",
    "# Loading data\n",
    "#\n",
    "_config = convert_flags_to_dict(FLAGS)\n",
    "_config[\"is_dev\"] = False\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "print('Pre-processing data...')\n",
    "tmp = _config[\"feature\"].split('+')\n",
    "features = []\n",
    "for f in tmp:\n",
    "    if f == 'upos':\n",
    "        features.append(Features.upos)\n",
    "\n",
    "    elif f == 'deprel':\n",
    "        features.append(Features.deprel)\n",
    "\n",
    "train_files = [cwd + '/data/GA/train.cupt'] if _config[\"is_dev\"] else []\n",
    "train_sents, train_labels = load_dataset(\n",
    "    train_files, features, cnn=True)\n",
    "\n",
    "# validation/dev dataset\n",
    "dev_files = [cwd + '/data/GA/dev.cupt'] if _config[\"is_dev\"] else []\n",
    "dev_sents, dev_labels = load_dataset(\n",
    "    dev_files, features, cnn=True, train=False)\n",
    "\n",
    "train_data, dev_data, (max_len, n_tokens) = pre_process_data(\n",
    "    (train_sents, train_labels), (dev_sents, dev_labels),\n",
    "    seed=SEED, cnn=True)\n",
    "\n",
    "_x_train, _x_val, _y_train, _y_val = train_data\n",
    "_x_dev, _y_dev = dev_data\n",
    "\n",
    "_config[\"x_train\"] = _x_train\n",
    "_config[\"x_val\"] = _x_val\n",
    "_config[\"y_train\"] = _y_train\n",
    "_config[\"y_val\"] = _y_val\n",
    "\n",
    "_config[\"x_dev\"] = _x_dev\n",
    "_config[\"y_dev\"] = _y_dev\n",
    "\n",
    "_config[\"n_tokens\"] = n_tokens\n",
    "_config[\"max_len\"] = _y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(config):\n",
    "\n",
    "    model_name = build_cnn_name('sentlevel_cnn', config)\n",
    "\n",
    "    print(\"Building model...\")\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    # embedding\n",
    "    model.add(\n",
    "        tf.keras.layers.Embedding(\n",
    "            config[\"n_tokens\"] + 1,\n",
    "            config[\"embed_dim\"],\n",
    "            input_shape=(config[\"x_train\"].shape[1], config[\"x_train\"].shape[2]),\n",
    "            input_length=config[\"max_len\"],\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=tf.random_uniform_initializer(\n",
    "                minval=-config[\"init_scale\"], maxval=config[\"init_scale\"],\n",
    "                seed=SEED)))\n",
    "\n",
    "    shape = model.layers[0].output_shape\n",
    "\n",
    "    model.add(tf.keras.layers.Reshape((shape[1], shape[3], shape[2])))\n",
    "\n",
    "    if config[\"spatial_dropout\"]:\n",
    "        model.add(tf.keras.layers.SpatialDropout2D(config[\"emb_dropout\"]))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dropout(config[\"emb_dropout\"]))\n",
    "\n",
    "    for filters in config[\"filters\"]:\n",
    "        model.add(\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters,\n",
    "                config[\"ngram\"],\n",
    "                padding='valid',\n",
    "                activation='relu',\n",
    "            #    strides=1,\n",
    "                kernel_initializer=tf.random_uniform_initializer(\n",
    "                    minval=-config[\"init_scale\"], maxval=config[\"init_scale\"],\n",
    "                    seed=SEED)))\n",
    "        model.add(tf.keras.layers.MaxPooling2D())\n",
    "\n",
    "    if config[\"global_pooling\"] == 'average':\n",
    "        model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    elif config[\"global_pooling\"] == 'max':\n",
    "        model.add(tf.keras.layers.GlobalMaxPool2D())\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    for _ in range(config[\"n_layers\"]):\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(\n",
    "                config[\"dense_size\"],\n",
    "                activation='relu',\n",
    "                kernel_initializer=tf.random_uniform_initializer(\n",
    "                    minval=-config[\"init_scale\"],\n",
    "                    maxval=config[\"init_scale\"],\n",
    "                    seed=SEED)))\n",
    "        model.add(tf.keras.layers.Dropout(config[\"dropout\"]))\n",
    "\n",
    "    if config[\"output_size\"] == 1:\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(1,\n",
    "                                activation='sigmoid',\n",
    "                                kernel_initializer=tf.random_uniform_initializer(\n",
    "                                    minval=-config[\"init_scale\"],\n",
    "                                    maxval=config[\"init_scale\"],\n",
    "                                    seed=SEED)))\n",
    "        y_train = config[\"y_train\"]\n",
    "        y_val = config[\"y_val\"]\n",
    "    else:\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(\n",
    "                2, activation=config[\"output_activation\"],\n",
    "                kernel_initializer=tf.random_uniform_initializer(\n",
    "                    minval=-config[\"init_scale\"],\n",
    "                    maxval=config[\"init_scale\"],\n",
    "                    seed=SEED)))\n",
    "        y_train = tf.keras.utils.to_categorical(config[\"y_train\"])\n",
    "        y_val = tf.keras.utils.to_categorical(config[\"y_val\"])\n",
    "\n",
    "\n",
    "    if config[\"optimizer\"] == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam\n",
    "    elif config[\"optimizer\"] == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD\n",
    "\n",
    "\n",
    "    # compiling model\n",
    "    model.compile(loss=config[\"loss_function\"],\n",
    "                optimizer=optimizer(learning_rate=config[\"learning_rate\"],\n",
    "                                    clipnorm=config[\"clipnorm\"]),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    class_weights = None\n",
    "    if config[\"weighted_loss\"]:\n",
    "        weights = class_weight.compute_class_weight(\n",
    "            'balanced', np.array([0, 1]), np.array([i for i in train_labels]))\n",
    "        class_weights = {}\n",
    "\n",
    "        for i in range(weights.shape[0]):\n",
    "            class_weights[i] = weights[i]\n",
    "\n",
    "    print('Class weights: {}'.format(class_weights))\n",
    "\n",
    "\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(config[\"train_dir\"] +\n",
    "                                                    model_name,\n",
    "                                                    save_best_only=True)\n",
    "    callbacks = [MetricsReporterCallback(custom_validation_data=(config[\"x_val\"], y_val)), checkpoint]\n",
    "\n",
    "    if config[\"early_stop_patience\"] > 0:\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='loss',\n",
    "            min_delta=config[\"early_stop_delta\"],\n",
    "            patience=config[\"early_stop_patience\"])\n",
    "        callbacks.append(early_stop)\n",
    "\n",
    "    if config[\"log_tensorboard\"]:\n",
    "        tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=config[\"train_dir\"] + '/logs')\n",
    "        callbacks.append(tensorboard)\n",
    "\n",
    "    def lr_scheduler(epoch, lr): # pylint: disable=C0103\n",
    "        lr_decay = config[\"lr_decay\"]**max(epoch - config[\"start_decay\"], 0.0)\n",
    "        return lr * lr_decay\n",
    "\n",
    "    if config[\"start_decay\"] > 0:\n",
    "        lrate = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "        callbacks.append(lrate)\n",
    "\n",
    "\n",
    "    print('Train...')\n",
    "    model.fit(\n",
    "        config[\"x_train\"],\n",
    "        y_train,\n",
    "        class_weight=class_weights,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        epochs=config[\"max_epochs\"],\n",
    "        callbacks=callbacks,\n",
    "        verbose=2,\n",
    "        validation_data=(config[\"x_val\"], y_val))\n",
    "\n",
    "    # #####\n",
    "    # Evaluation time\n",
    "    #\n",
    "    evaluate(model, test_data=(config[\"x_dev\"], config[\"y_dev\"]))\n",
    "\n",
    "#     tune.track.log(\n",
    "#         accuracy=_results[\"accuracy\"],\n",
    "#         label0_precision=_results[\"0\"][\"precision\"],\n",
    "#         label0_recall=_results[\"0\"][\"recall\"],\n",
    "#         label0_f1_score=_results[\"0\"][\"f1-score\"],\n",
    "#         label0_support=_results[\"0\"][\"support\"],\n",
    "#         label1_precision=_results[\"1\"][\"precision\"],\n",
    "#         label1_recall=_results[\"1\"][\"recall\"],\n",
    "#         label1_f1_score=_results[\"1\"][\"f1-score\"],\n",
    "#         label1_support=_results[\"1\"][\"support\"],\n",
    "#         macro_precision=_results[\"macro avg\"][\"precision\"],\n",
    "#         macro_recall=_results[\"macro avg\"][\"recall\"],\n",
    "#         macro_f1_score=_results[\"macro avg\"][\"f1-score\"],\n",
    "#         macro_support=_results[\"macro avg\"][\"support\"],\n",
    "#         weighted_precision=_results[\"weighted avg\"][\"precision\"],\n",
    "#         weighted_recall=_results[\"weighted avg\"][\"recall\"],\n",
    "#         weighted_f1_score=_results[\"weighted avg\"][\"f1-score\"],\n",
    "#         weighted_support=_results[\"weighted avg\"][\"support\"])\n",
    "\n",
    "#     return _results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"embed_dim\": hp.choice(\"embed_dim\", [10, 20, 30, 50, 75, 100]),\n",
    "    \"emb_dropout\": hp.uniform(\"emb_dropout\", 0.0, 0.9),\n",
    "    \"dropout\": hp.uniform(\"dropout\", 0.0, 0.9),\n",
    "    \"spatial_dropout\": hp.choice(\"spatial_dropout\", [True, False]),\n",
    "    \"init_scale\": hp.loguniform(\"init_scale\", np.log(1e-2), np.log(1e-1)),\n",
    "    \"n_layers\": hp.choice(\"n_layers\", [1, 2, 3, 4, 5]),\n",
    "    \"dense_size\": hp.choice(\"dense_size\", [10, 20, 30, 50, 75, 100]),\n",
    "    \"max_epochs\": hp.choice(\"max_epochs\", [10, 20, 30, 50]),\n",
    "    \"early_stop_delta\": hp.choice(\"early_stop_delta\", [0.001, 0.0001]),\n",
    "    \"early_stop_patience\": hp.choice(\"early_stop_patience\", [10, 20]),\n",
    "    \"optimizer\": hp.choice(\"optimizer\", ['sgd', 'adam', 'rmsprop']),\n",
    "    \"output_activation\": hp.choice(\"output_activation\", ['sigmoid', 'softmax']),\n",
    "    \"feature\":  hp.choice(\"feature\", [\"upos+deprel\", \"upos\", \"deprel\"]),\n",
    "    \"filters\": hp.choice(\"filters\", [[128], [128, 64, 32], [64, 32], [128, 32]]),\n",
    "    \"ngram\": hp.choice(\"ngram\", [1, 2, 3, 4, 5]),\n",
    "    \"global_pooling\": hp.choice(\"global_pooling\", ['', 'max', 'average']),\n",
    "    \"clipnorm\": hp.choice(\"clipnorm\", [0.5, 1.0, 2.5, 5.0, 10.0]),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(1e-4), np.log(1e-0)),\n",
    "    \"optimizer\": hp.choice(\"optimizer\", ['sgd', 'adam', 'rmsprop']),\n",
    "    \"batch_size\": hp.choice(\"batch_size\", [20, 24, 32, 64, 128]),\n",
    "}\n",
    "\n",
    "\n",
    "_config.update({\n",
    "    \"threads\": 2,\n",
    "    \"output_size\": 2,\n",
    "    \"start_decay\": 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()     # Restart Ray defensively in case the ray connection is lost.\n",
    "ray.init(num_cpus=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tune.run_experiments(\n",
    "    tune.Experiment(\n",
    "        run=train_model,        \n",
    "        name=\"tune-cnn\",\n",
    "        config=_config,\n",
    "         stop={\n",
    "            \"keras_info/label1_f1_score\": 0.9,\n",
    "            \"training_iteration\": 10**8\n",
    "        },\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 2,\n",
    "            \"gpu\": 0\n",
    "        },\n",
    "        num_samples=10,\n",
    "        checkpoint_freq=0,\n",
    "        checkpoint_at_end=False),\n",
    "    scheduler=AsyncHyperBandScheduler(\n",
    "        time_attr=\"epoch\",\n",
    "        metric=\"keras_info/label1_f1_score\",\n",
    "        mode=\"max\",\n",
    "        max_t=400,\n",
    "        grace_period=20),\n",
    "    search_alg=HyperOptSearch(\n",
    "            search_space,\n",
    "            metric=\"keras_info/label1_f1_score\",\n",
    "            mode=\"max\",\n",
    "            random_state_seed=SEED,\n",
    "            points_to_evaluate=[{\n",
    "                \"embed_dim\": 2,\n",
    "                \"emb_dropout\": 0.1,\n",
    "                \"dropout\": 0.1,\n",
    "                \"spatial_dropout\": 0,\n",
    "                \"init_scale\": 0.05,\n",
    "                \"n_layers\": 0,\n",
    "                \"dense_size\": 3,\n",
    "                \"max_epochs\": 3,\n",
    "                \"early_stop_delta\": 0,\n",
    "                \"early_stop_patience\": 0,\n",
    "                \"optimizer\": 1,\n",
    "                \"output_activation\": 0,\n",
    "                \"feature\":  0,\n",
    "                \"filters\": 0,\n",
    "                \"ngram\": 2,\n",
    "                \"global_pooling\": 1,\n",
    "                \"clipnorm\": 1,\n",
    "                \"learning_rate\": 0.0001,\n",
    "                \"optimizer\": 1,\n",
    "                \"batch_size\": 2,\n",
    "            }]),\n",
    "    verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dataframe().to_csv(_config[\"train_dir\"] + '/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "21842"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
