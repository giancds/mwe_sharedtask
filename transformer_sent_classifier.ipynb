{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 63051,\n",
      "  \"iopub_port\": 63052,\n",
      "  \"stdin_port\": 63053,\n",
      "  \"control_port\": 63055,\n",
      "  \"hb_port\": 63054,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"88f96dd4-a6dec9b03f3505fe8e2ddb95\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-fa16572b-ef4a-40c4-b92e-d143035687a5.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible models\n",
    "\n",
    "`bert-base-multilingual-cased`: (New, recommended) 12-layer, 768-hidden, 12-heads, 110M parameters. Trained on cased text in the top 104 languages with the largest Wikipedias\n",
    "\n",
    "`xlm-mlm-100-1280`: 16-layer, 1280-hidden, 16-heads XLM model trained with MLM (Masked Language Modeling) on 100 languages.\n",
    "\n",
    "`distilbert-base-multilingual-cased`: 6-layer, 768-hidden, 12-heads, 134M parameters The multilingual DistilBERT model distilled from the Multilingual BERT model bert-base-multilingual-cased checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BASE_DIR = os.path.expanduser(\"~\")     # this will point to the user's home\n",
    "TRAIN_DIR = BASE_DIR +  \"/ray_results\"\n",
    "\n",
    "\n",
    "model_type = 'distilbert-base-multilingual-cased'\n",
    "# model_type = 'bert-base-multilingual-cased'\n",
    "with open('/Users/gian/Documents/research/mwe_sharedtask/data/{}.embdata.pkl'.format(model_type), 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66338, 768) (66338,)\n",
      "(4330, 768) (4330,)\n"
     ]
    }
   ],
   "source": [
    "codes = ['DE', 'GA', 'HI', 'PT', 'ZH']\n",
    "\n",
    "\n",
    "x_train = np.concatenate([data[code]['x_train'] for code in codes], axis=0)\n",
    "y_train = np.concatenate([data[code]['y_train'] for code in codes], axis=0)\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "x_dev = np.concatenate([data[code]['x_dev'] for code in codes], axis=0)\n",
    "y_dev = np.concatenate([data[code]['y_dev'] for code in codes], axis=0)\n",
    "print(x_dev.shape, y_dev.shape)\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "knn_space = {\n",
    "    'model': hp.choice('model', [KNeighborsClassifier]),\n",
    "    'n_neighbors': hp.randint('n_neighbors', 1, 10),\n",
    "    'weights':  hp.choice('weights', ['uniform', 'distance']),\n",
    "    'p':  hp.randint('p', 1, 2) * 1,\n",
    "    'n_jobs': hp.choice('n_jobs', [-1])\n",
    "}\n",
    "\n",
    "lsvm_space = {\n",
    "    'model': hp.choice('model', [LinearSVC]),\n",
    "    'penalty':hp.choice('penalty', ['l1', 'l2']),\n",
    "    'loss':  hp.choice('loss', ['hinge', 'squared_hinge']),\n",
    "    'C': hp.loguniform('C', np.log(1e-6), np.log(1e+4)),\n",
    "    'class_weight': hp.choice('class_weight', ['balanced']),\n",
    "    'random_state': hp.choice('random_state', [SEED]),\n",
    "    'dual': hp.choice('dual', [True, False])\n",
    "}\n",
    "\n",
    "\n",
    "svm_space = {\n",
    "    'model': hp.choice('model', [SVC]),\n",
    "    'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'degree': hp.randint('degree', 1, 3) * 1,\n",
    "    'gamma': hp.choice('gamma', ['scale', 'auto']),\n",
    "    'C': hp.loguniform('C', np.log(1e-6), np.log(1e+4)),\n",
    "    'class_weight': hp.choice('class_weight', ['balanced']),\n",
    "    'random_state': hp.choice('random_state', [SEED]),\n",
    "}\n",
    "\n",
    "lreg_space = {\n",
    "    'model': hp.choice('model', [LogisticRegression]),\n",
    "    'penalty': hp.choice('penalty', ['l1', 'l2']),\n",
    "    'C': hp.loguniform('C', np.log(1e-6), np.log(1e+4)),\n",
    "    'solver': hp.choice('solver', ['liblinear']),\n",
    "    'n_jobs': hp.choice('n_jobs', [-1])\n",
    "}\n",
    "\n",
    "adab_space = {\n",
    "    'model': hp.choice('model', [AdaBoostClassifier]),\n",
    "    'base_estimator': hp.choice('base_estimator', [DecisionTreeClassifier(max_depth=hp.randint('max_depth', 1, 10))]),\n",
    "    'n_estimators': hp.randint('n_estimators', 1, 100) * 5,\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.1, 1.0),\n",
    "    'random_state': hp.choice('random_state', [SEED]),     \n",
    "}\n",
    "\n",
    "hist_space = {\n",
    "    'model': hp.choice('model', [HistGradientBoostingClassifier]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.1, 1.0),\n",
    "    'max_leaf_nodes': hp.randint('max_leaf_nodes', 2, 100) * 1,\n",
    "    'max_depth': hp.randint('max_depth', 2, 100) * 1,\n",
    "    'min_samples_leaf': hp.randint('min_samples_leaf', 1, 50) * 1,\n",
    "    'l2_regularization': hp.uniform('l2_regularization', 0.0, 10.0),\n",
    "    'max_bins': hp.randint('max_bins', 1, 60) * 5,\n",
    "    'random_state': hp.choice('random_state', [SEED]), \n",
    "}\n",
    "\n",
    "rf_space = {\n",
    "    'model': hp.choice('model', [RandomForestClassifier]),\n",
    "    'n_estimators': hp.randint('n_estimators', 1, 100) * 5,\n",
    "    'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "    'max_leaf_nodes': hp.randint('max_leaf_nodes', 2, 100) * 1,\n",
    "    'max_depth': hp.randint('max_depth', 2, 100) * 1,\n",
    "    'min_samples_leaf': hp.randint('min_samples_leaf', 1, 50) * 1,\n",
    "    'min_samples_split': hp.randint('min_samples_split', 1, 10) * 1,\n",
    "    'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2', None]),\n",
    "    'random_state': hp.choice('random_state', [SEED]), \n",
    "    'n_jobs': hp.choice('n_jobs', [-1])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'classifier': hp.choice(\n",
    "        'classifier', [\n",
    "            knn_space,\n",
    "            lsvm_space,\n",
    "            svm_space,\n",
    "            lreg_space,\n",
    "            adab_space,\n",
    "            hist_space,\n",
    "            rf_space]\n",
    "        )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': {'C': 123.36800787123339,\n",
       "  'class_weight': 'balanced',\n",
       "  'degree': 1,\n",
       "  'gamma': 'scale',\n",
       "  'kernel': 'sigmoid',\n",
       "  'model': sklearn.svm._classes.SVC,\n",
       "  'random_state': 42}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sample = sample(search_space)\n",
    "_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sample = _sample['classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 123.36800787123339,\n",
       " 'class_weight': 'balanced',\n",
       " 'degree': 1,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'sigmoid',\n",
       " 'model': sklearn.svm._classes.SVC,\n",
       " 'random_state': 42}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = _sample.pop('model')\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf(**_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=123.36800787123339, class_weight='balanced', degree=1, kernel='sigmoid',\n",
       "    random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=123.36800787123339, class_weight='balanced', degree=1, kernel='sigmoid',\n",
       "    random_state=42)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
